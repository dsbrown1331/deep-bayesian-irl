{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'StrippedNet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1e80f25a5849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#from run_test import *\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mStrippedNet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbeddingNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrex_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'StrippedNet'"
     ]
    }
   ],
   "source": [
    "#make sure it uses the custom baselines package\n",
    "import sys\n",
    "sys.path.insert(0,'./baselines/')\n",
    "sys.path.append(\".\")\n",
    "\n",
    "import argparse\n",
    "# coding: utf-8\n",
    "\n",
    "# Take as input a compressed pretrained network or run T_REX before hand\n",
    "# Then run MCMC and save posterior chain\n",
    "\n",
    "\n",
    "import pickle\n",
    "import copy\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from run_test import *\n",
    "from StrippedNet import EmbeddingNet\n",
    "from baselines.common.trex_utils import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mean_map_noop_demos(env, env_name, agent, mean_path):\n",
    "    demonstrations = []\n",
    "    learning_returns = []\n",
    "    learning_rewards = []\n",
    "    for model_path in [mean_path]:\n",
    "\n",
    "        agent.load(model_path)\n",
    "        episode_count = 2\n",
    "        for i in range(episode_count):\n",
    "            done = False\n",
    "            traj = []\n",
    "            gt_rewards = []\n",
    "            r = 0\n",
    "\n",
    "            ob = env.reset()\n",
    "            steps = 0\n",
    "            acc_reward = 0\n",
    "            while steps < 10000:\n",
    "                action = agent.act(ob, r, done)\n",
    "                ob, r, done, _ = env.step(action)\n",
    "                if args.render:\n",
    "                    env.render()\n",
    "                ob_processed = preprocess(ob, env_name)\n",
    "                #ob_processed = ob_processed[0] #get rid of first dimension ob.shape = (1,84,84,4)\n",
    "                traj.append(ob_processed)\n",
    "\n",
    "                gt_rewards.append(r[0])\n",
    "                steps += 1\n",
    "                acc_reward += r[0]\n",
    "                if done:\n",
    "                    print(\"checkpoint: {}, steps: {}, return: {}\".format(model_path, steps,acc_reward))\n",
    "                    break\n",
    "            print(\"traj length\", len(traj))\n",
    "            print(\"demo length\", len(demonstrations))\n",
    "            demonstrations.append(traj)\n",
    "            learning_returns.append(acc_reward)\n",
    "            learning_rewards.append(gt_rewards)\n",
    "\n",
    "    #add no-op demos\n",
    "    done = False\n",
    "    traj = []\n",
    "    gt_rewards = []\n",
    "    r = 0\n",
    "\n",
    "    ob = env.reset()\n",
    "    steps = 0\n",
    "    acc_reward = 0\n",
    "    while steps < 5000:\n",
    "        action = 0#agent.act(ob, r, done)\n",
    "        ob, r, done, _ = env.step(action)\n",
    "        ob_processed = preprocess(ob, env_name)\n",
    "        #ob_processed = ob_processed[0] #get rid of first dimension ob.shape = (1,84,84,4)\n",
    "        traj.append(ob_processed)\n",
    "\n",
    "        gt_rewards.append(r[0])\n",
    "        steps += 1\n",
    "        acc_reward += r[0]\n",
    "        if done:\n",
    "            print(\"checkpoint: {}, steps: {}, return: {}\".format(\"noop\", steps,acc_reward))\n",
    "            break\n",
    "    print(\"noop traj length\", len(traj))\n",
    "    print(\"demo length\", len(demonstrations))\n",
    "    demonstrations.append(traj)\n",
    "    learning_returns.append(acc_reward)\n",
    "    learning_rewards.append(gt_rewards)\n",
    "\n",
    "\n",
    "    return demonstrations, learning_returns, learning_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_novice_demos(env, env_name, agent, model_dir):\n",
    "    checkpoint_min = 50\n",
    "    checkpoint_max = 600\n",
    "    checkpoint_step = 50\n",
    "    checkpoints = []\n",
    "    if env_name == \"enduro\":\n",
    "        checkpoint_min = 3100\n",
    "        checkpoint_max = 3650\n",
    "    elif env_name == \"seaquest\":\n",
    "        checkpoint_min = 10\n",
    "        checkpoint_max = 65\n",
    "        checkpoint_step = 5\n",
    "    for i in range(checkpoint_min, checkpoint_max + checkpoint_step, checkpoint_step):\n",
    "        if i < 10:\n",
    "            checkpoints.append('0000' + str(i))\n",
    "        elif i < 100:\n",
    "            checkpoints.append('000' + str(i))\n",
    "        elif i < 1000:\n",
    "            checkpoints.append('00' + str(i))\n",
    "        elif i < 10000:\n",
    "            checkpoints.append('0' + str(i))\n",
    "    print(checkpoints)\n",
    "\n",
    "\n",
    "\n",
    "    demonstrations = []\n",
    "    learning_returns = []\n",
    "    learning_rewards = []\n",
    "    for checkpoint in checkpoints:\n",
    "\n",
    "        model_path = model_dir + env_name + \"_25/\" + checkpoint\n",
    "        if env_name == \"seaquest\":\n",
    "            model_path = model_dir + env_name + \"_5/\" + checkpoint\n",
    "\n",
    "        agent.load(model_path)\n",
    "        episode_count = 1\n",
    "        for i in range(episode_count):\n",
    "            done = False\n",
    "            traj = []\n",
    "            gt_rewards = []\n",
    "            r = 0\n",
    "\n",
    "            ob = env.reset()\n",
    "            steps = 0\n",
    "            acc_reward = 0\n",
    "            while True:\n",
    "                action = agent.act(ob, r, done)\n",
    "                ob, r, done, _ = env.step(action)\n",
    "                ob_processed = preprocess(ob, env_name)\n",
    "                #ob_processed = ob_processed[0] #get rid of first dimension ob.shape = (1,84,84,4)\n",
    "                traj.append(ob_processed)\n",
    "\n",
    "                gt_rewards.append(r[0])\n",
    "                steps += 1\n",
    "                acc_reward += r[0]\n",
    "                if done:\n",
    "                    print(\"checkpoint: {}, steps: {}, return: {}\".format(checkpoint, steps,acc_reward))\n",
    "                    break\n",
    "            print(\"traj length\", len(traj))\n",
    "            print(\"demo length\", len(demonstrations))\n",
    "            demonstrations.append(traj)\n",
    "            learning_returns.append(acc_reward)\n",
    "            learning_rewards.append(gt_rewards)\n",
    "\n",
    "    return demonstrations, learning_returns, learning_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mcmc_likelihood_data(demonstrations):\n",
    "    '''create all pairwise rankings given list of sorted demonstrations'''\n",
    "    training_obs = []\n",
    "    training_labels = []\n",
    "    num_demos = len(demonstrations)\n",
    "    for i in range(num_demos):\n",
    "        for j in range(i+1,num_demos):\n",
    "            #print(i,j)\n",
    "            traj_i = demonstrations[i]\n",
    "            traj_j = demonstrations[j]\n",
    "            label = 1\n",
    "            training_obs.append((traj_i, traj_j))\n",
    "            training_labels.append(label)\n",
    "\n",
    "    return training_obs, training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_reward_sequence(net, traj):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    rewards_from_obs = []\n",
    "    with torch.no_grad():\n",
    "        for s in traj:\n",
    "            r = net.cum_return(torch.from_numpy(np.array(s)).float().to(device)).item()\n",
    "            rewards_from_obs.append(r)\n",
    "    return rewards_from_obs\n",
    "\n",
    "def predict_traj_return(net, traj):\n",
    "    return sum(predict_reward_sequence(net, traj))\n",
    "\n",
    "def print_traj_returns(reward_net, demonstrations):\n",
    "    #print out predicted cumulative returns and actual returns\n",
    "    with torch.no_grad():\n",
    "        pred_returns = [predict_traj_return(reward_net, traj) for traj in demonstrations]\n",
    "    for i, p in enumerate(pred_returns):\n",
    "        print(i,p,sorted_returns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_linearized_pairwise_ranking_loss(last_layer, pairwise_prefs, demo_cnts, confidence=1):\n",
    "    '''use (i,j) indices and precomputed feature counts to do faster pairwise ranking loss'''\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "    #print(device)\n",
    "    #don't need any gradients\n",
    "    with torch.no_grad():\n",
    "\n",
    "        #do matrix multiply with last layer of network and the demo_cnts\n",
    "        #print(list(reward_net.fc2.parameters()))\n",
    "        linear = last_layer.weight.data  #not using bias\n",
    "        #print(linear)\n",
    "        #print(bias)\n",
    "        weights = linear.squeeze() #append bias and weights from last fc layer together\n",
    "        #print('weights',weights)\n",
    "        #print('demo_cnts', demo_cnts)\n",
    "        demo_returns = confidence * torch.mv(demo_cnts, weights)\n",
    "\n",
    "        #positivity prior\n",
    "        if demo_returns[0] < 0.0:\n",
    "            return torch.Tensor([-float(\"Inf\")])\n",
    "\n",
    "\n",
    "        loss_criterion = nn.CrossEntropyLoss(reduction='sum') #sum up losses\n",
    "        cum_log_likelihood = 0.0\n",
    "        outputs = torch.zeros(len(pairwise_prefs),2) #each row is a new pair of returns\n",
    "        for p, ppref in enumerate(pairwise_prefs):\n",
    "            i,j = ppref\n",
    "            outputs[p,:] = torch.tensor([demo_returns[i], demo_returns[j]])\n",
    "        labels = torch.ones(len(pairwise_prefs)).long()\n",
    "\n",
    "        #outputs = outputs.unsqueeze(0)\n",
    "        #print(outputs)\n",
    "        #print(labels)\n",
    "        cum_log_likelihood = -loss_criterion(outputs, labels)\n",
    "            #if labels == 0:\n",
    "            #    log_likelihood = torch.log(return_i/(return_i + return_j))\n",
    "            #else:\n",
    "            #    log_likelihood = torch.log(return_j/(return_i + return_j))\n",
    "            #print(\"ll\",log_likelihood)\n",
    "            #cum_log_likelihood += log_likelihood\n",
    "    return cum_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(reward_net, demonstrations, num_trials, stdev = 0.1):\n",
    "    '''hill climbing random search'''\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    best_likelihood = -np.inf\n",
    "    best_reward = copy.deepcopy(reward_net)\n",
    "    #create the pairwise rankings for loss calculations\n",
    "    demo_pairs, preference_labels = create_mcmc_likelihood_data(demonstrations)\n",
    "    for i in range(num_trials):\n",
    "        print()\n",
    "        print(\"trial\", i)\n",
    "        reward_net_proposal = copy.deepcopy(best_reward)\n",
    "        #add random noise to weights\n",
    "        with torch.no_grad():\n",
    "            for param in reward_net_proposal.parameters():\n",
    "                param.add_(torch.randn(param.size()).to(device) * stdev)\n",
    "        #print_traj_returns(reward_net_proposal, demonstrations)\n",
    "        cum_loglik = calc_pairwise_ranking_loss(reward_net_proposal, demo_pairs, preference_labels)\n",
    "        print(\"pair-wise ranking loglik\", cum_loglik)\n",
    "        if cum_loglik > best_likelihood:\n",
    "            best_likelihood = cum_loglik\n",
    "            best_reward = copy.deepcopy(reward_net_proposal)\n",
    "            print(\"updating best to \", best_likelihood)\n",
    "        else:\n",
    "            print(\"rejecting\")\n",
    "    return best_reward\n",
    "\n",
    "def generate_feature_counts(demos, reward_net):\n",
    "    feature_cnts = torch.zeros(len(demos), reward_net.fc2.in_features) #no bias\n",
    "    for i in range(len(demos)):\n",
    "        traj = np.array(demos[i])\n",
    "        traj = torch.from_numpy(traj).float().to(device)\n",
    "        #print(len(trajectory))\n",
    "        feature_cnts[i,:] = reward_net.state_features(traj).squeeze().float().to(device)\n",
    "    return feature_cnts.to(device)\n",
    "\n",
    "def get_weight_vector(last_layer):\n",
    "    '''take fc2 layer and return numpy array of weights and bias'''\n",
    "    linear = last_layer.weight.data\n",
    "    #print(linear)\n",
    "    #print(bias)\n",
    "    with torch.no_grad():\n",
    "        weights = linear.squeeze().cpu().numpy()\n",
    "    return weights\n",
    "\n",
    "def write_weights_likelihood(last_layer, loglik, file_writer):\n",
    "    if args.debug:\n",
    "        print(\"writing weights\")\n",
    "    #convert last layer to numpy array\n",
    "    np_weights = get_weight_vector(last_layer)\n",
    "    for w in np_weights:\n",
    "        file_writer.write(str(w)+\",\")\n",
    "    file_writer.write(str(loglik.item()) + \"\\n\")\n",
    "\n",
    "def compute_l1(last_layer):\n",
    "    linear = last_layer.weight.data\n",
    "    #print(linear)\n",
    "    #print(bias)\n",
    "    with torch.no_grad():\n",
    "        weights = linear.squeeze().cpu().numpy()\n",
    "    #print(\"output\", np.sum(np.abs(weights)))\n",
    "    return np.sum(np.abs(weights))\n",
    "\n",
    "def compute_l2(last_layer):\n",
    "    linear = last_layer.weight.data\n",
    "    #print(linear)\n",
    "    #print(bias)\n",
    "    with torch.no_grad():\n",
    "        weights = linear.squeeze().cpu().numpy()\n",
    "    #print(\"output\", np.sum(np.abs(weights)))\n",
    "    return np.linalg.norm(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc_map_search(reward_net, demonstrations, pairwise_prefs, demo_cnts, num_steps, step_stdev, weight_output_filename, weight_init):\n",
    "    '''run metropolis hastings MCMC and record weights in chain'''\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    demo_pairs, preference_labels = create_mcmc_likelihood_data(demonstrations)\n",
    "\n",
    "    writer = open(weight_output_filename,'w')\n",
    "\n",
    "    last_layer = reward_net.fc2\n",
    "\n",
    "    if weight_init == \"randn\":\n",
    "        with torch.no_grad():\n",
    "            print(last_layer.parameters())\n",
    "            linear = last_layer.weight.data\n",
    "\n",
    "            linear.add_(torch.randn(linear.size()).to(device) * step_stdev)\n",
    "    elif \":\" in weight_init:\n",
    "        print(weight_init.strip().split(':'))\n",
    "        weight_index, init_weight = weight_init.strip().split(':')\n",
    "        init_weight = float(init_weight)\n",
    "        weight_index = int(weight_index)\n",
    "        print(\"weight index\", weight_index, \"init weight\", init_weight)\n",
    "        #initialize with one hot in weight_init position (i.e. initialize in one of the corners of the unit 1-norm sphere)\n",
    "        with torch.no_grad():\n",
    "            #get size of weight vector\n",
    "            num_weights = reward_net.fc2.in_features  #not including the bias weight\n",
    "            #set up initial weights\n",
    "            new_linear = torch.zeros(num_weights)\n",
    "            new_bias = torch.zeros(1)\n",
    "            if weight_index < num_weights:\n",
    "                new_linear[weight_index] = init_weight\n",
    "            else:\n",
    "                new_bias[0] = init_weight\n",
    "            #unsqueeze since nn.Linear wants a 2-d tensor for weights\n",
    "            new_linear = new_linear.unsqueeze(0)\n",
    "            print(\"new linear\", new_linear)\n",
    "            print(\"new bias\", new_bias)\n",
    "            with torch.no_grad():\n",
    "                #print(last_layer.weight)\n",
    "                #print(last_layer.bias)\n",
    "                #print(last_layer.weight.data)\n",
    "                #print(last_layer.bias.data)\n",
    "                last_layer.weight.data = new_linear.to(device)\n",
    "                last_layer.bias.data = new_bias.to(device)\n",
    "    else:\n",
    "        print(\"not a valid weight initialization for MCMC\")\n",
    "        sys.exit()\n",
    "\n",
    "    #normalize the weight vector to have unit 1-norm...why not unit 2-norm, WCFB won't work without expert...I guess we could do D-REX and estimate\n",
    "    l2_norm = np.array([compute_l2(last_layer)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        linear = last_layer.weight.data\n",
    "        print(last_layer.parameters())\n",
    "        linear.div_(torch.from_numpy(l2_norm).float().to(device))\n",
    "\n",
    "    if args.debug:\n",
    "        print(\"normalized last layer\", compute_l2(last_layer))\n",
    "        print(\"weights\", get_weight_vector(last_layer))\n",
    "\n",
    "    #import time\n",
    "    #start_t = time.time()\n",
    "    #starting_loglik = calc_pairwise_ranking_loss(reward_net, demo_pairs, preference_labels)\n",
    "    #end_t = time.time()\n",
    "    #print(\"slow likelihood\", starting_loglik, \"time\", 1000*(end_t - start_t))\n",
    "    #start_t = time.time()\n",
    "    starting_loglik = calc_linearized_pairwise_ranking_loss(last_layer, pairwise_prefs, demo_cnts)\n",
    "    #end_t = time.time()\n",
    "    #print(\"new fast? likelihood\", new_starting_loglik, \"time\", 1000*(end_t - start_t))\n",
    "    #print(bunnY)\n",
    "\n",
    "    map_loglik = starting_loglik\n",
    "    map_reward = copy.deepcopy(reward_net.fc2)\n",
    "\n",
    "    cur_reward = copy.deepcopy(reward_net.fc2)\n",
    "    cur_loglik = starting_loglik\n",
    "\n",
    "\n",
    "\n",
    "    reject_cnt = 0\n",
    "    accept_cnt = 0\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        if args.debug:\n",
    "            print(\"step\", i)\n",
    "        #take a proposal step\n",
    "        proposal_reward = copy.deepcopy(cur_reward)\n",
    "        #add random noise to weights of last layer\n",
    "        with torch.no_grad():\n",
    "            for param in proposal_reward.parameters():\n",
    "                param.add_(torch.randn(param.size()).to(device) * step_stdev)\n",
    "        l2_norm = np.array([compute_l2(proposal_reward)])\n",
    "        #normalize the weight vector...\n",
    "        with torch.no_grad():\n",
    "            for param in proposal_reward.parameters():\n",
    "                param.div_(torch.from_numpy(l2_norm).float().to(device))\n",
    "        if args.debug:\n",
    "            print(\"normalized last layer\", compute_l2(proposal_reward))\n",
    "        #debugging info\n",
    "        #print_traj_returns(proposal_reward, demonstrations)\n",
    "        #calculate prob of proposal\n",
    "        prop_loglik = calc_linearized_pairwise_ranking_loss(proposal_reward, pairwise_prefs, demo_cnts)\n",
    "        if args.debug:\n",
    "            print(\"proposal loglik\", prop_loglik.item())\n",
    "            print(\"cur loglik\", cur_loglik.item())\n",
    "        if prop_loglik > cur_loglik:\n",
    "            #print()\n",
    "            #accept always\n",
    "            if args.debug:\n",
    "                print(\"accept\")\n",
    "            accept_cnt += 1\n",
    "            cur_reward = copy.deepcopy(proposal_reward)\n",
    "            cur_loglik = prop_loglik\n",
    "\n",
    "            #check if this is best so far\n",
    "            if prop_loglik > map_loglik:\n",
    "                map_loglik = prop_loglik\n",
    "                map_reward = copy.deepcopy(proposal_reward)\n",
    "                print()\n",
    "                print(\"step\", i)\n",
    "\n",
    "                print(\"proposal loglik\", prop_loglik.item())\n",
    "\n",
    "                print(\"updating map to \", prop_loglik)\n",
    "        else:\n",
    "            #accept with prob exp(prop_loglik - cur_loglik)\n",
    "            if np.random.rand() < torch.exp(prop_loglik - cur_loglik).item():\n",
    "                #print()\n",
    "                #print(\"step\", i)\n",
    "                if args.debug:\n",
    "                    print(\"proposal loglik\", prop_loglik.item())\n",
    "                    print(\"probabilistic accept\")\n",
    "                accept_cnt += 1\n",
    "                cur_reward = copy.deepcopy(proposal_reward)\n",
    "                cur_loglik = prop_loglik\n",
    "            else:\n",
    "                #reject and stick with cur_reward\n",
    "                if args.debug:\n",
    "                    print(\"reject\")\n",
    "                reject_cnt += 1\n",
    "\n",
    "        #save chain of weights\n",
    "        write_weights_likelihood(cur_reward, cur_loglik, writer)\n",
    "    print(\"num rejects\", reject_cnt)\n",
    "    print(\"num accepts\", accept_cnt)\n",
    "    writer.close()\n",
    "    return map_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=None)\n",
    "parser.add_argument('--env_name', default='breakout', help='Select the environment name to run, i.e. pong')\n",
    "parser.add_argument('--map_reward_model_path', default='../le', help=\"name and location for learned model params, e.g. ../mcmc_data/breakout_map.params\")\n",
    "parser.add_argument('--seed', default=0, help=\"random seed for experiments\")\n",
    "parser.add_argument('--models_dir', default = \"../learning-rewards-of-learners/learner/models/\", help=\"path to directory that contains checkpoint models for demos are stored\")\n",
    "parser.add_argument('--num_mcmc_steps', default=2000, type = int, help=\"number of proposals to generate for MCMC\")\n",
    "parser.add_argument('--mcmc_step_size', default = 0.005, type=float, help=\"proposal step is gaussian with zero mean and mcmc_step_size stdev\")\n",
    "parser.add_argument('--pretrained_network', help='path to pretrained network weights to form \\phi(s) using all but last layer')\n",
    "parser.add_argument('--weight_outputfile', help='filename including path to write the chain to')\n",
    "parser.add_argument('--debug', help='use fewer demos to speed things up while debugging', action='store_true' )\n",
    "parser.add_argument('--plot', help='plot out the feature counts over time for demos', action='store_true' )\n",
    "parser.add_argument('--weight_init', help=\"defaults to randn, specify integer value to start in a corner of L1-sphere\", default=\"randn\")\n",
    "parser.add_argument('--encoding_dims', help=\"size of latent space\", type=int)\n",
    "parser.add_argument('--mean_path', default=\"/home/dsbrown/Code/deep-bayesian-irl/learned_policies/breakout_linear_mean\")\n",
    "parser.add_argument('--map_path', default=\"/home/dsbrown/Code/deep-bayesian-irl/learned_policies/breakout_linear_map\")\n",
    "parser.add_argument('--render', action='store_true')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "env_name = args.env_name\n",
    "if env_name == \"spaceinvaders\":\n",
    "    env_id = \"SpaceInvadersNoFrameskip-v4\"\n",
    "elif env_name == \"mspacman\":\n",
    "    env_id = \"MsPacmanNoFrameskip-v4\"\n",
    "elif env_name == \"videopinball\":\n",
    "    env_id = \"VideoPinballNoFrameskip-v4\"\n",
    "elif env_name == \"beamrider\":\n",
    "    env_id = \"BeamRiderNoFrameskip-v4\"\n",
    "else:\n",
    "    env_id = env_name[0].upper() + env_name[1:] + \"NoFrameskip-v4\"\n",
    "\n",
    "env_type = \"atari\"\n",
    "print(env_type)\n",
    "#set seeds\n",
    "seed = int(args.seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "stochastic = True\n",
    "\n",
    "\n",
    "env = make_vec_env(env_id, 'atari', 1, seed,\n",
    "                   wrapper_kwargs={\n",
    "                       'clip_rewards':False,\n",
    "                       'episode_life':False,\n",
    "                   })\n",
    "\n",
    "\n",
    "env = VecFrameStack(env, 4)\n",
    "agent = PPO2Agent(env, env_type, stochastic)\n",
    "\n",
    "\n",
    "orig_demonstrations, orig_learning_returns, orig_learning_rewards = generate_novice_demos(env, env_name, agent, args.models_dir)\n",
    "more_demonstrations, more_learning_returns, more_learning_rewards = generate_mean_map_noop_demos(env, env_name, agent, args.mean_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstrations = orig_demonstrations + more_demonstrations\n",
    "learning_returns = orig_learning_returns + more_learning_returns\n",
    "learning_rewards = orig_learning_rewards + more_learning_rewards\n",
    "\n",
    "#sort the demonstrations according to ground truth reward to simulate ranked demos\n",
    "\n",
    "print([a[0] for a in zip(learning_returns, demonstrations)])\n",
    "demonstrations = [x for _, x in sorted(zip(learning_returns,demonstrations), key=lambda pair: pair[0])]\n",
    "\n",
    "sorted_returns = sorted(learning_returns)\n",
    "print(sorted_returns)\n",
    "\n",
    "\n",
    "# Now we download a pretrained network to form \\phi(s) the state features where the reward is now w^T \\phi(s)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "reward_net = EmbeddingNet(args.encoding_dims)\n",
    "reward_net.load_state_dict(torch.load(args.pretrained_network, map_location=device))\n",
    "#reinitialize last layer\n",
    "num_features = reward_net.fc2.in_features\n",
    "\n",
    "print(\"reward is linear combination of \", num_features, \"features\")\n",
    "reward_net.fc2 = nn.Linear(num_features, 1, bias=False) #last layer just outputs the scalar reward = w^T \\phi(s)\n",
    "reward_net.to(device)\n",
    "#freeze all weights so there are no gradients (we'll manually update the last layer via proposals so no grads required)\n",
    "for param in reward_net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#get num_demos by num_features + 1 (bias) numpy array with (un-discounted) feature counts from pretrained network\n",
    "demo_cnts = generate_feature_counts(demonstrations, reward_net)\n",
    "print(\"demo counts\")\n",
    "print(demo_cnts)\n",
    "if args.plot:\n",
    "    plotable_cnts = demo_cnts.cpu().numpy()\n",
    "    import matplotlib.pyplot as plt\n",
    "    for f in range(num_features):\n",
    "        #plt.figure(f)\n",
    "        if plotable_cnts[0,f] < plotable_cnts[-1,f]: #increasing\n",
    "            plt.figure(0)\n",
    "            plt.plot(plotable_cnts[:,f], '-o', label='feature ' + str(f))\n",
    "            plt.legend()\n",
    "        elif plotable_cnts[0,f] > plotable_cnts[-1,f]: #decreasing\n",
    "            plt.figure(1)\n",
    "            plt.plot(plotable_cnts[:,f], '-o', label='feature ' + str(f))\n",
    "            plt.legend()\n",
    "        else: #unknown\n",
    "            plt.figure(2)\n",
    "            plt.plot(plotable_cnts[:,f], '-o', label='feature ' + str(f))\n",
    "            plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    #print(demo_cnts.shape)\n",
    "\n",
    "#just need index tuples (i,j) denoting j is preferred to i. Assuming all pairwise prefs for now\n",
    "#check if really better, there might be ties!\n",
    "pairwise_prefs = []\n",
    "for i in range(len(demonstrations)):\n",
    "    for j in range(i+1, len(demonstrations)):\n",
    "        if sorted_returns[i] < sorted_returns[j]:\n",
    "            pairwise_prefs.append((i,j))\n",
    "        else: # they are equal\n",
    "            print(\"not using equal prefs\", i, j, sorted_returns[i], sorted_returns[j])\n",
    "            #pairwise_prefs.append((i,j))\n",
    "            #pairwise_prefs.append((j,i))\n",
    "\n",
    "\n",
    "#run random search over weights\n",
    "#best_reward = random_search(reward_net, demonstrations, 40, stdev = 0.01)\n",
    "best_reward_lastlayer = mcmc_map_search(reward_net, demonstrations, pairwise_prefs, demo_cnts, args.num_mcmc_steps, args.mcmc_step_size, args.weight_outputfile, args.weight_init)\n",
    "#turn this into a full network\n",
    "best_reward = EmbeddingNet(args.encoding_dims)\n",
    "#best_reward.fc2 = nn.Linear(num_features, 1, bias=False)\n",
    "best_reward.load_state_dict(torch.load(args.pretrained_network, map_location=device))\n",
    "best_reward.fc2 = best_reward_lastlayer\n",
    "\n",
    "best_reward.to(device)\n",
    "#print(best_reward.state_dict())\n",
    "#save best reward network\n",
    "torch.save(best_reward.state_dict(), args.map_reward_model_path)\n",
    "demo_pairs, preference_labels = create_mcmc_likelihood_data(demonstrations)\n",
    "print_traj_returns(best_reward, demonstrations)\n",
    "\n",
    "\n",
    "#add random"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
