{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsbrown/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dsbrown/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dsbrown/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dsbrown/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dsbrown/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dsbrown/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/dsbrown/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dsbrown/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dsbrown/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dsbrown/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dsbrown/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dsbrown/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/openai-2020-02-04-16-47-01-809131\n"
     ]
    }
   ],
   "source": [
    "#make sure it uses the custom baselines package\n",
    "import sys\n",
    "sys.path.insert(0,'./baselines/')\n",
    "\n",
    "import argparse\n",
    "# coding: utf-8\n",
    "\n",
    "# Take as input a compressed pretrained network or run T_REX before hand\n",
    "# Then run MCMC and save posterior chain\n",
    "\n",
    "\n",
    "import pickle\n",
    "import copy\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from run_test import *\n",
    "from StrippedNet import EmbeddingNet\n",
    "from baselines.common.trex_utils import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mean_map_noop_demos(env, env_name, agent, mean_path, map_path):\n",
    "    demonstrations = []\n",
    "    learning_returns = []\n",
    "    learning_rewards = []\n",
    "    for model_path in [mean_path, map_path]:\n",
    "\n",
    "        agent.load(model_path)\n",
    "        episode_count = 2\n",
    "        for i in range(episode_count):\n",
    "            done = False\n",
    "            traj = []\n",
    "            gt_rewards = []\n",
    "            r = 0\n",
    "\n",
    "            ob = env.reset()\n",
    "            steps = 0\n",
    "            acc_reward = 0\n",
    "            while steps < 8000:\n",
    "                action = agent.act(ob, r, done)\n",
    "                ob, r, done, _ = env.step(action)\n",
    "                if render:\n",
    "                    env.render()\n",
    "                ob_processed = preprocess(ob, env_name)\n",
    "                #ob_processed = ob_processed[0] #get rid of first dimension ob.shape = (1,84,84,4)\n",
    "                traj.append(ob_processed)\n",
    "\n",
    "                gt_rewards.append(r[0])\n",
    "                steps += 1\n",
    "                acc_reward += r[0]\n",
    "                if done:\n",
    "                    \n",
    "                    break\n",
    "            print(\"checkpoint: {}, steps: {}, return: {}\".format(model_path, steps,acc_reward))\n",
    "            print(\"traj length\", len(traj))\n",
    "            print(\"demo length\", len(demonstrations))\n",
    "            \n",
    "            \n",
    "            #check if it went too long and segment it into good and bad\n",
    "            if steps == 8000:\n",
    "                traj_rewards = gt_rewards\n",
    "                for i in range(len(traj)-1,-1,-1):\n",
    "                    if traj_rewards[i] > 0:\n",
    "                        indx = i+1\n",
    "                        print(indx, \"last time when scored\")\n",
    "                        break\n",
    "                print(\"total return after\", np.sum(traj_rewards[indx:]))\n",
    "                good_segment = traj[:indx+1]\n",
    "                good_rewards = traj_rewards[:indx+1]\n",
    "                bad_segment = traj[indx+1:]\n",
    "                bad_rewards = traj_rewards[indx+1:]\n",
    "                print(\"good segment length\", len(good_segment), \"reward\", np.sum(good_rewards))\n",
    "                print(\"bad segment length\", len(bad_segment), \"rewards\", np.sum(bad_rewards))\n",
    "            \n",
    "                print(\"adding to demonstrations\")\n",
    "                demonstrations.append(good_segment)\n",
    "                learning_returns.append(np.sum(good_rewards))\n",
    "                learning_rewards.append(good_rewards)\n",
    "                demonstrations.append(bad_segment)\n",
    "                learning_returns.append(np.sum(bad_rewards))\n",
    "                learning_rewards.append(bad_rewards)\n",
    "            else:\n",
    "                demonstrations.append(traj)\n",
    "                learning_returns.append(acc_reward)\n",
    "                learning_rewards.append(gt_rewards)\n",
    "\n",
    "    #add no-op demos\n",
    "    done = False\n",
    "    traj = []\n",
    "    gt_rewards = []\n",
    "    r = 0\n",
    "\n",
    "    ob = env.reset()\n",
    "    steps = 0\n",
    "    acc_reward = 0\n",
    "    while steps < 5000:\n",
    "        action = 0#agent.act(ob, r, done)\n",
    "        ob, r, done, _ = env.step(action)\n",
    "        ob_processed = preprocess(ob, env_name)\n",
    "        #ob_processed = ob_processed[0] #get rid of first dimension ob.shape = (1,84,84,4)\n",
    "        traj.append(ob_processed)\n",
    "\n",
    "        gt_rewards.append(r[0])\n",
    "        steps += 1\n",
    "        acc_reward += r[0]\n",
    "        if done:\n",
    "            print(\"checkpoint: {}, steps: {}, return: {}\".format(\"noop\", steps,acc_reward))\n",
    "            break\n",
    "    print(\"noop traj length\", len(traj))\n",
    "    print(\"demo length\", len(demonstrations))\n",
    "    demonstrations.append(traj)\n",
    "    learning_returns.append(acc_reward)\n",
    "    learning_rewards.append(gt_rewards)\n",
    "\n",
    "\n",
    "    return demonstrations, learning_returns, learning_rewards\n",
    "\n",
    "\n",
    "\n",
    "def generate_novice_demos(env, env_name, agent, model_dir):\n",
    "    checkpoint_min = 50\n",
    "    checkpoint_max = 600\n",
    "    checkpoint_step = 50\n",
    "    checkpoints = []\n",
    "    if env_name == \"enduro\":\n",
    "        checkpoint_min = 3100\n",
    "        checkpoint_max = 3650\n",
    "    elif env_name == \"seaquest\":\n",
    "        checkpoint_min = 10\n",
    "        checkpoint_max = 65\n",
    "        checkpoint_step = 5\n",
    "    for i in range(checkpoint_min, checkpoint_max + checkpoint_step, checkpoint_step):\n",
    "        if i < 10:\n",
    "            checkpoints.append('0000' + str(i))\n",
    "        elif i < 100:\n",
    "            checkpoints.append('000' + str(i))\n",
    "        elif i < 1000:\n",
    "            checkpoints.append('00' + str(i))\n",
    "        elif i < 10000:\n",
    "            checkpoints.append('0' + str(i))\n",
    "    print(checkpoints)\n",
    "\n",
    "\n",
    "\n",
    "    demonstrations = []\n",
    "    learning_returns = []\n",
    "    learning_rewards = []\n",
    "    for checkpoint in checkpoints:\n",
    "\n",
    "        model_path = model_dir + env_name + \"_25/\" + checkpoint\n",
    "        if env_name == \"seaquest\":\n",
    "            model_path = model_dir + env_name + \"_5/\" + checkpoint\n",
    "\n",
    "        agent.load(model_path)\n",
    "        episode_count = 1\n",
    "        for i in range(episode_count):\n",
    "            done = False\n",
    "            traj = []\n",
    "            gt_rewards = []\n",
    "            r = 0\n",
    "\n",
    "            ob = env.reset()\n",
    "            steps = 0\n",
    "            acc_reward = 0\n",
    "            while True:\n",
    "                action = agent.act(ob, r, done)\n",
    "                ob, r, done, _ = env.step(action)\n",
    "                ob_processed = preprocess(ob, env_name)\n",
    "                #ob_processed = ob_processed[0] #get rid of first dimension ob.shape = (1,84,84,4)\n",
    "                traj.append(ob_processed)\n",
    "\n",
    "                gt_rewards.append(r[0])\n",
    "                steps += 1\n",
    "                acc_reward += r[0]\n",
    "                if done:\n",
    "                    print(\"checkpoint: {}, steps: {}, return: {}\".format(checkpoint, steps,acc_reward))\n",
    "                    break\n",
    "            print(\"traj length\", len(traj))\n",
    "            print(\"demo length\", len(demonstrations))\n",
    "            demonstrations.append(traj)\n",
    "            learning_returns.append(acc_reward)\n",
    "            learning_rewards.append(gt_rewards)\n",
    "\n",
    "    return demonstrations, learning_returns, learning_rewards\n",
    "\n",
    "\n",
    "\n",
    "def create_mcmc_likelihood_data(demonstrations):\n",
    "    '''create all pairwise rankings given list of sorted demonstrations'''\n",
    "    training_obs = []\n",
    "    training_labels = []\n",
    "    num_demos = len(demonstrations)\n",
    "    for i in range(num_demos):\n",
    "        for j in range(i+1,num_demos):\n",
    "            #print(i,j)\n",
    "            traj_i = demonstrations[i]\n",
    "            traj_j = demonstrations[j]\n",
    "            label = 1\n",
    "            training_obs.append((traj_i, traj_j))\n",
    "            training_labels.append(label)\n",
    "\n",
    "    return training_obs, training_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_reward_sequence(net, traj):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    rewards_from_obs = []\n",
    "    with torch.no_grad():\n",
    "        for s in traj:\n",
    "            r = net.cum_return(torch.from_numpy(np.array(s)).float().to(device)).item()\n",
    "            rewards_from_obs.append(r)\n",
    "    return rewards_from_obs\n",
    "\n",
    "def predict_traj_return(net, traj):\n",
    "    return sum(predict_reward_sequence(net, traj))\n",
    "\n",
    "def print_traj_returns(reward_net, demonstrations):\n",
    "    #print out predicted cumulative returns and actual returns\n",
    "    with torch.no_grad():\n",
    "        pred_returns = [predict_traj_return(reward_net, traj) for traj in demonstrations]\n",
    "    for i, p in enumerate(pred_returns):\n",
    "        print(i,p,sorted_returns[i])\n",
    "\n",
    "def calc_linearized_pairwise_ranking_loss(last_layer, pairwise_prefs, demo_cnts, confidence=1):\n",
    "    '''use (i,j) indices and precomputed feature counts to do faster pairwise ranking loss'''\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "    #print(device)\n",
    "    #don't need any gradients\n",
    "    with torch.no_grad():\n",
    "\n",
    "        #do matrix multiply with last layer of network and the demo_cnts\n",
    "        #print(list(reward_net.fc2.parameters()))\n",
    "        linear = last_layer.weight.data  #not using bias\n",
    "        #print(linear)\n",
    "        #print(bias)\n",
    "        weights = linear.squeeze() #append bias and weights from last fc layer together\n",
    "        #print('weights',weights)\n",
    "        #print('demo_cnts', demo_cnts)\n",
    "        demo_returns = confidence * torch.mv(demo_cnts, weights)\n",
    "\n",
    "        #positivity prior\n",
    "        if demo_returns[0] < 0.0:\n",
    "            return torch.Tensor([-float(\"Inf\")])\n",
    "\n",
    "\n",
    "        loss_criterion = nn.CrossEntropyLoss(reduction='sum') #sum up losses\n",
    "        cum_log_likelihood = 0.0\n",
    "        outputs = torch.zeros(len(pairwise_prefs),2) #each row is a new pair of returns\n",
    "        for p, ppref in enumerate(pairwise_prefs):\n",
    "            i,j = ppref\n",
    "            outputs[p,:] = torch.tensor([demo_returns[i], demo_returns[j]])\n",
    "        labels = torch.ones(len(pairwise_prefs)).long()\n",
    "\n",
    "        #outputs = outputs.unsqueeze(0)\n",
    "        #print(outputs)\n",
    "        #print(labels)\n",
    "        cum_log_likelihood = -loss_criterion(outputs, labels)\n",
    "            #if labels == 0:\n",
    "            #    log_likelihood = torch.log(return_i/(return_i + return_j))\n",
    "            #else:\n",
    "            #    log_likelihood = torch.log(return_j/(return_i + return_j))\n",
    "            #print(\"ll\",log_likelihood)\n",
    "            #cum_log_likelihood += log_likelihood\n",
    "    return cum_log_likelihood\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def random_search(reward_net, demonstrations, num_trials, stdev = 0.1):\n",
    "    '''hill climbing random search'''\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    best_likelihood = -np.inf\n",
    "    best_reward = copy.deepcopy(reward_net)\n",
    "    #create the pairwise rankings for loss calculations\n",
    "    demo_pairs, preference_labels = create_mcmc_likelihood_data(demonstrations)\n",
    "    for i in range(num_trials):\n",
    "        print()\n",
    "        print(\"trial\", i)\n",
    "        reward_net_proposal = copy.deepcopy(best_reward)\n",
    "        #add random noise to weights\n",
    "        with torch.no_grad():\n",
    "            for param in reward_net_proposal.parameters():\n",
    "                param.add_(torch.randn(param.size()).to(device) * stdev)\n",
    "        #print_traj_returns(reward_net_proposal, demonstrations)\n",
    "        cum_loglik = calc_pairwise_ranking_loss(reward_net_proposal, demo_pairs, preference_labels)\n",
    "        print(\"pair-wise ranking loglik\", cum_loglik)\n",
    "        if cum_loglik > best_likelihood:\n",
    "            best_likelihood = cum_loglik\n",
    "            best_reward = copy.deepcopy(reward_net_proposal)\n",
    "            print(\"updating best to \", best_likelihood)\n",
    "        else:\n",
    "            print(\"rejecting\")\n",
    "    return best_reward\n",
    "\n",
    "def generate_feature_counts(demos, reward_net):\n",
    "    feature_cnts = torch.zeros(len(demos), reward_net.fc2.in_features) #no bias\n",
    "    for i in range(len(demos)):\n",
    "        traj = np.array(demos[i])\n",
    "        traj = torch.from_numpy(traj).float().to(device)\n",
    "        #print(len(trajectory))\n",
    "        feature_cnts[i,:] = reward_net.state_features(traj).squeeze().float().to(device)\n",
    "    return feature_cnts.to(device)\n",
    "\n",
    "def get_weight_vector(last_layer):\n",
    "    '''take fc2 layer and return numpy array of weights and bias'''\n",
    "    linear = last_layer.weight.data\n",
    "    #print(linear)\n",
    "    #print(bias)\n",
    "    with torch.no_grad():\n",
    "        weights = linear.squeeze().cpu().numpy()\n",
    "    return weights\n",
    "\n",
    "def write_weights_likelihood(last_layer, loglik, file_writer):\n",
    "    if debug:\n",
    "        print(\"writing weights\")\n",
    "    #convert last layer to numpy array\n",
    "    np_weights = get_weight_vector(last_layer)\n",
    "    for w in np_weights:\n",
    "        file_writer.write(str(w)+\",\")\n",
    "    file_writer.write(str(loglik.item()) + \"\\n\")\n",
    "\n",
    "def compute_l1(last_layer):\n",
    "    linear = last_layer.weight.data\n",
    "    #print(linear)\n",
    "    #print(bias)\n",
    "    with torch.no_grad():\n",
    "        weights = linear.squeeze().cpu().numpy()\n",
    "    #print(\"output\", np.sum(np.abs(weights)))\n",
    "    return np.sum(np.abs(weights))\n",
    "\n",
    "def compute_l2(last_layer):\n",
    "    linear = last_layer.weight.data\n",
    "    #print(linear)\n",
    "    #print(bias)\n",
    "    with torch.no_grad():\n",
    "        weights = linear.squeeze().cpu().numpy()\n",
    "    #print(\"output\", np.sum(np.abs(weights)))\n",
    "    return np.linalg.norm(weights)\n",
    "\n",
    "\n",
    "def mcmc_map_search(reward_net, demonstrations, pairwise_prefs, demo_cnts, num_steps, step_stdev, weight_output_filename, weight_init):\n",
    "    '''run metropolis hastings MCMC and record weights in chain'''\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    demo_pairs, preference_labels = create_mcmc_likelihood_data(demonstrations)\n",
    "\n",
    "    writer = open(weight_output_filename,'w')\n",
    "\n",
    "    last_layer = reward_net.fc2\n",
    "    #set the initial weights to be the signs of the feature counts\n",
    "    new_data = last_layer.weight.data.cpu().numpy()\n",
    "    new_data = np.sign(new_data)\n",
    "    new_data /= np.linalg.norm(new_data)\n",
    "    print(new_data)\n",
    "    last_layer.weight.data = torch.from_numpy(new_data).to(device)\n",
    "    input()\n",
    "    if weight_init == \"randn\":\n",
    "        with torch.no_grad():\n",
    "            print(last_layer.parameters())\n",
    "            linear = last_layer.weight.data\n",
    "\n",
    "            linear.add_(torch.randn(linear.size()).to(device) * step_stdev)\n",
    "    elif \":\" in weight_init:\n",
    "        print(weight_init.strip().split(':'))\n",
    "        weight_index, init_weight = weight_init.strip().split(':')\n",
    "        init_weight = float(init_weight)\n",
    "        weight_index = int(weight_index)\n",
    "        print(\"weight index\", weight_index, \"init weight\", init_weight)\n",
    "        #initialize with one hot in weight_init position (i.e. initialize in one of the corners of the unit 1-norm sphere)\n",
    "        with torch.no_grad():\n",
    "            #get size of weight vector\n",
    "            num_weights = reward_net.fc2.in_features  #not including the bias weight\n",
    "            #set up initial weights\n",
    "            new_linear = torch.zeros(num_weights)\n",
    "            new_bias = torch.zeros(1)\n",
    "            if weight_index < num_weights:\n",
    "                new_linear[weight_index] = init_weight\n",
    "            else:\n",
    "                new_bias[0] = init_weight\n",
    "            #unsqueeze since nn.Linear wants a 2-d tensor for weights\n",
    "            new_linear = new_linear.unsqueeze(0)\n",
    "            print(\"new linear\", new_linear)\n",
    "            print(\"new bias\", new_bias)\n",
    "            with torch.no_grad():\n",
    "                #print(last_layer.weight)\n",
    "                #print(last_layer.bias)\n",
    "                #print(last_layer.weight.data)\n",
    "                #print(last_layer.bias.data)\n",
    "                last_layer.weight.data = new_linear.to(device)\n",
    "                last_layer.bias.data = new_bias.to(device)\n",
    "    else:\n",
    "        print(\"not a valid weight initialization for MCMC\")\n",
    "        sys.exit()\n",
    "\n",
    "    #normalize the weight vector to have unit 1-norm...why not unit 2-norm, WCFB won't work without expert...I guess we could do D-REX and estimate\n",
    "    l2_norm = np.array([compute_l2(last_layer)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        linear = last_layer.weight.data\n",
    "        print(last_layer.parameters())\n",
    "        linear.div_(torch.from_numpy(l2_norm).float().to(device))\n",
    "\n",
    "    if debug:\n",
    "        print(\"normalized last layer\", compute_l2(last_layer))\n",
    "        print(\"weights\", get_weight_vector(last_layer))\n",
    "\n",
    "    #import time\n",
    "    #start_t = time.time()\n",
    "    #starting_loglik = calc_pairwise_ranking_loss(reward_net, demo_pairs, preference_labels)\n",
    "    #end_t = time.time()\n",
    "    #print(\"slow likelihood\", starting_loglik, \"time\", 1000*(end_t - start_t))\n",
    "    #start_t = time.time()\n",
    "    starting_loglik = calc_linearized_pairwise_ranking_loss(last_layer, pairwise_prefs, demo_cnts)\n",
    "    #end_t = time.time()\n",
    "    #print(\"new fast? likelihood\", new_starting_loglik, \"time\", 1000*(end_t - start_t))\n",
    "    #print(bunnY)\n",
    "\n",
    "    map_loglik = starting_loglik\n",
    "    map_reward = copy.deepcopy(reward_net.fc2)\n",
    "\n",
    "    cur_reward = copy.deepcopy(reward_net.fc2)\n",
    "    cur_loglik = starting_loglik\n",
    "\n",
    "\n",
    "\n",
    "    reject_cnt = 0\n",
    "    accept_cnt = 0\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        if debug:\n",
    "            print(\"step\", i)\n",
    "        #take a proposal step\n",
    "        proposal_reward = copy.deepcopy(cur_reward)\n",
    "        #add random noise to weights of last layer\n",
    "        with torch.no_grad():\n",
    "            for param in proposal_reward.parameters():\n",
    "                param.add_(torch.randn(param.size()).to(device) * step_stdev)\n",
    "        l2_norm = np.array([compute_l2(proposal_reward)])\n",
    "        #normalize the weight vector...\n",
    "        with torch.no_grad():\n",
    "            for param in proposal_reward.parameters():\n",
    "                param.div_(torch.from_numpy(l2_norm).float().to(device))\n",
    "        if debug:\n",
    "            print(\"normalized last layer\", compute_l2(proposal_reward))\n",
    "        #debugging info\n",
    "        #print_traj_returns(proposal_reward, demonstrations)\n",
    "        #calculate prob of proposal\n",
    "        prop_loglik = calc_linearized_pairwise_ranking_loss(proposal_reward, pairwise_prefs, demo_cnts)\n",
    "        if debug:\n",
    "            print(\"proposal loglik\", prop_loglik.item())\n",
    "            print(\"cur loglik\", cur_loglik.item())\n",
    "        if prop_loglik > cur_loglik:\n",
    "            #print()\n",
    "            #accept always\n",
    "            if debug:\n",
    "                print(\"accept\")\n",
    "            accept_cnt += 1\n",
    "            cur_reward = copy.deepcopy(proposal_reward)\n",
    "            cur_loglik = prop_loglik\n",
    "\n",
    "            #check if this is best so far\n",
    "            if prop_loglik > map_loglik:\n",
    "                map_loglik = prop_loglik\n",
    "                map_reward = copy.deepcopy(proposal_reward)\n",
    "                print()\n",
    "                print(\"step\", i)\n",
    "\n",
    "                print(\"proposal loglik\", prop_loglik.item())\n",
    "\n",
    "                print(\"updating map to \", prop_loglik)\n",
    "        else:\n",
    "            #accept with prob exp(prop_loglik - cur_loglik)\n",
    "            if np.random.rand() < torch.exp(prop_loglik - cur_loglik).item():\n",
    "                #print()\n",
    "                #print(\"step\", i)\n",
    "                if debug:\n",
    "                    print(\"proposal loglik\", prop_loglik.item())\n",
    "                    print(\"probabilistic accept\")\n",
    "                accept_cnt += 1\n",
    "                cur_reward = copy.deepcopy(proposal_reward)\n",
    "                cur_loglik = prop_loglik\n",
    "            else:\n",
    "                #reject and stick with cur_reward\n",
    "                if debug:\n",
    "                    print(\"reject\")\n",
    "                reject_cnt += 1\n",
    "\n",
    "        #save chain of weights\n",
    "        write_weights_likelihood(cur_reward, cur_loglik, writer)\n",
    "    print(\"num rejects\", reject_cnt)\n",
    "    print(\"num accepts\", accept_cnt)\n",
    "    writer.close()\n",
    "    return map_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name ='breakout'\n",
    "map_reward_model_path = \"../mcmc_data/test.params\"\n",
    "seed = 0 #, help=\"random seed for experiments\")\n",
    "models_dir = \"../../learning-rewards-of-learners/learner/models/\"#, help=\"path to directory that contains checkpoint models for demos are stored\")\n",
    "mcmc_step_size = 0.005#', default = 0.005, type=float, help=\"proposal step is gaussian with zero mean and mcmc_step_size stdev\")\n",
    "pretrained_network = \"../pretrained_networks/auxloss/breakout_64_all.params_stripped.params\"# ', help='path to pretrained network weights to form \\phi(s) using all but last layer')\n",
    "weight_outputfile=\"../mcmc_data/test.txt\"#', help='filename including path to write the chain to')\n",
    "debug = False#', help='use fewer demos to speed things up while debugging', action='store_true' )\n",
    "plot = False#', help='plot out the feature counts over time for demos', action='store_true' )\n",
    "weight_init = \"randn\"#', help=\"defaults to randn, specify integer value to start in a corner of L1-sphere\", default=\"randn\")\n",
    "encoding_dims = 64 #', help=\"size of latent space\", type=int)\n",
    "mean_path = \"/home/dsbrown/Code/deep-bayesian-irl/learned_policies/breakout_linear_mean\"\n",
    "map_path= \"/home/dsbrown/Code/deep-bayesian-irl/learned_policies/breakout_linear_map\"\n",
    "render = False#', action='store_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atari\n",
      "making vec env\n",
      "WARNING:tensorflow:From ./baselines/baselines/common/misc_util.py:79: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "dummy vec\n",
      "WARNING:tensorflow:From ./baselines/baselines/common/tf_util.py:54: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From ./baselines/baselines/common/tf_util.py:64: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From ./baselines/baselines/common/tf_util.py:71: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
      "\n",
      "WARNING:tensorflow:From ./baselines/baselines/ppo2/model.py:31: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From ./baselines/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From ./baselines/baselines/common/policies.py:43: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From ./baselines/baselines/ppo2/model.py:97: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dsbrown/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "env_name = env_name\n",
    "if env_name == \"spaceinvaders\":\n",
    "    env_id = \"SpaceInvadersNoFrameskip-v4\"\n",
    "elif env_name == \"mspacman\":\n",
    "    env_id = \"MsPacmanNoFrameskip-v4\"\n",
    "elif env_name == \"videopinball\":\n",
    "    env_id = \"VideoPinballNoFrameskip-v4\"\n",
    "elif env_name == \"beamrider\":\n",
    "    env_id = \"BeamRiderNoFrameskip-v4\"\n",
    "else:\n",
    "    env_id = env_name[0].upper() + env_name[1:] + \"NoFrameskip-v4\"\n",
    "\n",
    "env_type = \"atari\"\n",
    "print(env_type)\n",
    "#set seeds\n",
    "seed = int(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "stochastic = True\n",
    "\n",
    "\n",
    "env = make_vec_env(env_id, 'atari', 1, seed,\n",
    "                   wrapper_kwargs={\n",
    "                       'clip_rewards':False,\n",
    "                       'episode_life':False,\n",
    "                   })\n",
    "\n",
    "\n",
    "env = VecFrameStack(env, 4)\n",
    "agent = PPO2Agent(env, env_type, stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00050', '00100', '00150', '00200', '00250', '00300', '00350', '00400', '00450', '00500', '00550', '00600']\n",
      "checkpoint: 00050, steps: 478, return: 8.0\n",
      "traj length 478\n",
      "demo length 0\n",
      "checkpoint: 00100, steps: 137, return: 0.0\n",
      "traj length 137\n",
      "demo length 1\n",
      "checkpoint: 00150, steps: 264, return: 3.0\n",
      "traj length 264\n",
      "demo length 2\n",
      "checkpoint: 00200, steps: 413, return: 8.0\n",
      "traj length 413\n",
      "demo length 3\n",
      "checkpoint: 00250, steps: 510, return: 10.0\n",
      "traj length 510\n",
      "demo length 4\n",
      "checkpoint: 00300, steps: 731, return: 23.0\n",
      "traj length 731\n",
      "demo length 5\n",
      "checkpoint: 00350, steps: 640, return: 14.0\n",
      "traj length 640\n",
      "demo length 6\n",
      "checkpoint: 00400, steps: 705, return: 16.0\n",
      "traj length 705\n",
      "demo length 7\n",
      "checkpoint: 00450, steps: 725, return: 21.0\n",
      "traj length 725\n",
      "demo length 8\n",
      "checkpoint: 00500, steps: 739, return: 20.0\n",
      "traj length 739\n",
      "demo length 9\n",
      "checkpoint: 00550, steps: 720, return: 17.0\n",
      "traj length 720\n",
      "demo length 10\n",
      "checkpoint: 00600, steps: 815, return: 19.0\n",
      "traj length 815\n",
      "demo length 11\n",
      "checkpoint: /home/dsbrown/Code/deep-bayesian-irl/learned_policies/breakout_linear_mean, steps: 2581, return: 397.0\n",
      "traj length 2581\n",
      "demo length 0\n",
      "checkpoint: /home/dsbrown/Code/deep-bayesian-irl/learned_policies/breakout_linear_mean, steps: 3797, return: 417.0\n",
      "traj length 3797\n",
      "demo length 1\n",
      "checkpoint: /home/dsbrown/Code/deep-bayesian-irl/learned_policies/breakout_linear_map, steps: 8000, return: 431.0\n",
      "traj length 8000\n",
      "demo length 2\n",
      "2330 last time when scored\n",
      "total return after 0.0\n",
      "good segment length 2331 reward 431.0\n",
      "bad segment length 5669 rewards 0.0\n",
      "adding to demonstrations\n",
      "checkpoint: /home/dsbrown/Code/deep-bayesian-irl/learned_policies/breakout_linear_map, steps: 8000, return: 401.0\n",
      "traj length 8000\n",
      "demo length 4\n",
      "1559 last time when scored\n",
      "total return after 0.0\n",
      "good segment length 1560 reward 401.0\n",
      "bad segment length 6440 rewards 0.0\n",
      "adding to demonstrations\n",
      "noop traj length 5000\n",
      "demo length 6\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "orig_demonstrations, orig_learning_returns, orig_learning_rewards = generate_novice_demos(env, env_name, agent, models_dir)\n",
    "more_demonstrations, more_learning_returns, more_learning_rewards = generate_mean_map_noop_demos(env, env_name, agent, mean_path, map_path)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1560\n",
      "1559\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# print(len(more_demonstrations))\n",
    "# print(len(more_demonstrations[-3]))\n",
    "# traj = more_demonstrations[-3]\n",
    "# traj_rewards = more_learning_rewards[-3]\n",
    "# for i in range(len(traj)-1,-1,-1):\n",
    "#     if traj_rewards[i] > 0:\n",
    "#         indx = i+1\n",
    "#         print(indx)\n",
    "#         break\n",
    "# print(np.sum(traj_rewards[indx:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create a snippet for comparison\n",
    "# good_segment = traj[:indx+1]\n",
    "# bad_segment = traj[indx:]\n",
    "# print(len(good_segment))\n",
    "# print(len(bad_segment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bad_breakout_behavior():\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(traj[-1].shape)\n",
    "    for i in range(2000):\n",
    "        plt.imshow(traj[-1 - i][0])\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"../figs/breakout_cant_hit.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.0, 0.0, 3.0, 8.0, 10.0, 23.0, 14.0, 16.0, 21.0, 20.0, 17.0, 19.0, 397.0, 417.0, 431.0, 0.0, 401.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 3.0, 8.0, 8.0, 10.0, 14.0, 16.0, 17.0, 19.0, 20.0, 21.0, 23.0, 397.0, 401.0, 417.0, 431.0]\n"
     ]
    }
   ],
   "source": [
    "demonstrations = orig_demonstrations + more_demonstrations\n",
    "learning_returns = orig_learning_returns + more_learning_returns\n",
    "learning_rewards = orig_learning_rewards + more_learning_rewards\n",
    "\n",
    "#sort the demonstrations according to ground truth reward to simulate ranked demos\n",
    "\n",
    "print([a[0] for a in zip(learning_returns, demonstrations)])\n",
    "demonstrations = [x for _, x in sorted(zip(learning_returns,demonstrations), key=lambda pair: pair[0])]\n",
    "\n",
    "sorted_returns = sorted(learning_returns)\n",
    "print(sorted_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward is linear combination of  64 features\n",
      "demo counts\n",
      "tensor([[ 5.3298e+01, -9.1161e+01,  6.6356e+01,  ...,  1.5951e+00,\n",
      "          1.9192e+01, -4.6382e+01],\n",
      "        [-4.3560e+03,  8.2327e+03, -4.6915e+03,  ..., -5.7918e+02,\n",
      "         -1.1762e+03,  3.8790e+03],\n",
      "        [-2.4695e+03,  4.7959e+03, -2.6950e+03,  ..., -2.2048e+02,\n",
      "         -6.3044e+02,  2.1376e+03],\n",
      "        ...,\n",
      "        [-3.6061e+02,  7.3072e+02, -3.8111e+02,  ..., -3.1567e+01,\n",
      "         -8.3099e+01,  3.2540e+02],\n",
      "        [-2.6640e+03,  4.8586e+03, -2.8732e+03,  ..., -1.6409e+02,\n",
      "         -7.1788e+02,  2.2514e+03],\n",
      "        [-1.3175e+03,  2.4406e+03, -1.4275e+03,  ..., -6.5463e+01,\n",
      "         -3.5764e+02,  1.1275e+03]], device='cuda:0')\n",
      "not using equal prefs 0 1 0.0 0.0\n",
      "not using equal prefs 0 2 0.0 0.0\n",
      "not using equal prefs 0 3 0.0 0.0\n",
      "not using equal prefs 1 2 0.0 0.0\n",
      "not using equal prefs 1 3 0.0 0.0\n",
      "not using equal prefs 2 3 0.0 0.0\n",
      "not using equal prefs 5 6 8.0 8.0\n",
      "[[ 0.125 -0.125  0.125  0.125 -0.125 -0.125  0.125  0.125  0.125  0.125\n",
      "   0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125  0.125 -0.125\n",
      "  -0.125 -0.125  0.125  0.125 -0.125  0.125  0.125 -0.125  0.125 -0.125\n",
      "  -0.125 -0.125 -0.125  0.125  0.125 -0.125  0.125 -0.125 -0.125  0.125\n",
      "   0.125  0.125 -0.125 -0.125  0.125 -0.125 -0.125  0.125 -0.125  0.125\n",
      "   0.125 -0.125 -0.125  0.125 -0.125  0.125 -0.125  0.125  0.125 -0.125\n",
      "   0.125 -0.125  0.125 -0.125]]\n",
      "\n",
      "<generator object Module.parameters at 0x7f10401e3bf8>\n",
      "<generator object Module.parameters at 0x7f10401e3bf8>\n",
      "\n",
      "step 1\n",
      "proposal loglik -112189.609375\n",
      "updating map to  tensor(-112189.6094)\n",
      "\n",
      "step 2\n",
      "proposal loglik -111890.375\n",
      "updating map to  tensor(-111890.3750)\n",
      "\n",
      "step 4\n",
      "proposal loglik -111827.015625\n",
      "updating map to  tensor(-111827.0156)\n",
      "\n",
      "step 6\n",
      "proposal loglik -111612.359375\n",
      "updating map to  tensor(-111612.3594)\n",
      "\n",
      "step 7\n",
      "proposal loglik -109304.6796875\n",
      "updating map to  tensor(-109304.6797)\n",
      "\n",
      "step 9\n",
      "proposal loglik -108430.515625\n",
      "updating map to  tensor(-108430.5156)\n",
      "\n",
      "step 11\n",
      "proposal loglik -108105.2734375\n",
      "updating map to  tensor(-108105.2734)\n",
      "\n",
      "step 16\n",
      "proposal loglik -107523.078125\n",
      "updating map to  tensor(-107523.0781)\n",
      "\n",
      "step 17\n",
      "proposal loglik -106001.875\n",
      "updating map to  tensor(-106001.8750)\n",
      "\n",
      "step 18\n",
      "proposal loglik -104744.671875\n",
      "updating map to  tensor(-104744.6719)\n",
      "\n",
      "step 19\n",
      "proposal loglik -104219.3671875\n",
      "updating map to  tensor(-104219.3672)\n",
      "\n",
      "step 22\n",
      "proposal loglik -102865.6875\n",
      "updating map to  tensor(-102865.6875)\n",
      "\n",
      "step 23\n",
      "proposal loglik -102232.0703125\n",
      "updating map to  tensor(-102232.0703)\n",
      "\n",
      "step 24\n",
      "proposal loglik -101032.8515625\n",
      "updating map to  tensor(-101032.8516)\n",
      "\n",
      "step 28\n",
      "proposal loglik -100808.390625\n",
      "updating map to  tensor(-100808.3906)\n",
      "\n",
      "step 30\n",
      "proposal loglik -100424.84375\n",
      "updating map to  tensor(-100424.8438)\n",
      "\n",
      "step 34\n",
      "proposal loglik -100277.8203125\n",
      "updating map to  tensor(-100277.8203)\n",
      "\n",
      "step 37\n",
      "proposal loglik -99440.2265625\n",
      "updating map to  tensor(-99440.2266)\n",
      "\n",
      "step 39\n",
      "proposal loglik -97537.6796875\n",
      "updating map to  tensor(-97537.6797)\n",
      "\n",
      "step 42\n",
      "proposal loglik -97166.5859375\n",
      "updating map to  tensor(-97166.5859)\n",
      "\n",
      "step 43\n",
      "proposal loglik -95882.828125\n",
      "updating map to  tensor(-95882.8281)\n",
      "\n",
      "step 46\n",
      "proposal loglik -95631.5234375\n",
      "updating map to  tensor(-95631.5234)\n",
      "\n",
      "step 47\n",
      "proposal loglik -94486.578125\n",
      "updating map to  tensor(-94486.5781)\n",
      "\n",
      "step 48\n",
      "proposal loglik -93844.90625\n",
      "updating map to  tensor(-93844.9062)\n",
      "\n",
      "step 50\n",
      "proposal loglik -93693.1171875\n",
      "updating map to  tensor(-93693.1172)\n",
      "\n",
      "step 52\n",
      "proposal loglik -91832.4453125\n",
      "updating map to  tensor(-91832.4453)\n",
      "\n",
      "step 53\n",
      "proposal loglik -91457.625\n",
      "updating map to  tensor(-91457.6250)\n",
      "\n",
      "step 54\n",
      "proposal loglik -89840.171875\n",
      "updating map to  tensor(-89840.1719)\n",
      "\n",
      "step 55\n",
      "proposal loglik -89742.015625\n",
      "updating map to  tensor(-89742.0156)\n",
      "\n",
      "step 56\n",
      "proposal loglik -89294.15625\n",
      "updating map to  tensor(-89294.1562)\n",
      "\n",
      "step 58\n",
      "proposal loglik -89021.171875\n",
      "updating map to  tensor(-89021.1719)\n",
      "\n",
      "step 61\n",
      "proposal loglik -88965.046875\n",
      "updating map to  tensor(-88965.0469)\n",
      "\n",
      "step 62\n",
      "proposal loglik -88580.0\n",
      "updating map to  tensor(-88580.)\n",
      "\n",
      "step 63\n",
      "proposal loglik -87714.7890625\n",
      "updating map to  tensor(-87714.7891)\n",
      "\n",
      "step 65\n",
      "proposal loglik -86993.078125\n",
      "updating map to  tensor(-86993.0781)\n",
      "\n",
      "step 66\n",
      "proposal loglik -86855.8828125\n",
      "updating map to  tensor(-86855.8828)\n",
      "\n",
      "step 67\n",
      "proposal loglik -86139.1875\n",
      "updating map to  tensor(-86139.1875)\n",
      "\n",
      "step 68\n",
      "proposal loglik -84832.1953125\n",
      "updating map to  tensor(-84832.1953)\n",
      "\n",
      "step 69\n",
      "proposal loglik -80941.078125\n",
      "updating map to  tensor(-80941.0781)\n",
      "\n",
      "step 70\n",
      "proposal loglik -79910.921875\n",
      "updating map to  tensor(-79910.9219)\n",
      "\n",
      "step 72\n",
      "proposal loglik -79414.4453125\n",
      "updating map to  tensor(-79414.4453)\n",
      "\n",
      "step 74\n",
      "proposal loglik -79324.3828125\n",
      "updating map to  tensor(-79324.3828)\n",
      "\n",
      "step 77\n",
      "proposal loglik -79005.3359375\n",
      "updating map to  tensor(-79005.3359)\n",
      "\n",
      "step 78\n",
      "proposal loglik -78006.40625\n",
      "updating map to  tensor(-78006.4062)\n",
      "\n",
      "step 79\n",
      "proposal loglik -76573.875\n",
      "updating map to  tensor(-76573.8750)\n",
      "\n",
      "step 82\n",
      "proposal loglik -76570.15625\n",
      "updating map to  tensor(-76570.1562)\n",
      "\n",
      "step 84\n",
      "proposal loglik -75238.2265625\n",
      "updating map to  tensor(-75238.2266)\n",
      "\n",
      "step 85\n",
      "proposal loglik -72836.4375\n",
      "updating map to  tensor(-72836.4375)\n",
      "\n",
      "step 86\n",
      "proposal loglik -71670.765625\n",
      "updating map to  tensor(-71670.7656)\n",
      "\n",
      "step 87\n",
      "proposal loglik -70141.546875\n",
      "updating map to  tensor(-70141.5469)\n",
      "\n",
      "step 91\n",
      "proposal loglik -69237.40625\n",
      "updating map to  tensor(-69237.4062)\n",
      "\n",
      "step 92\n",
      "proposal loglik -69234.765625\n",
      "updating map to  tensor(-69234.7656)\n",
      "\n",
      "step 94\n",
      "proposal loglik -67625.9765625\n",
      "updating map to  tensor(-67625.9766)\n",
      "\n",
      "step 96\n",
      "proposal loglik -66420.890625\n",
      "updating map to  tensor(-66420.8906)\n",
      "\n",
      "step 97\n",
      "proposal loglik -66330.890625\n",
      "updating map to  tensor(-66330.8906)\n",
      "\n",
      "step 98\n",
      "proposal loglik -65450.046875\n",
      "updating map to  tensor(-65450.0469)\n",
      "\n",
      "step 99\n",
      "proposal loglik -64324.765625\n",
      "updating map to  tensor(-64324.7656)\n",
      "\n",
      "step 100\n",
      "proposal loglik -63512.61328125\n",
      "updating map to  tensor(-63512.6133)\n",
      "\n",
      "step 101\n",
      "proposal loglik -63402.3984375\n",
      "updating map to  tensor(-63402.3984)\n",
      "\n",
      "step 106\n",
      "proposal loglik -62762.84765625\n",
      "updating map to  tensor(-62762.8477)\n",
      "\n",
      "step 108\n",
      "proposal loglik -61607.890625\n",
      "updating map to  tensor(-61607.8906)\n",
      "\n",
      "step 109\n",
      "proposal loglik -60688.5625\n",
      "updating map to  tensor(-60688.5625)\n",
      "\n",
      "step 110\n",
      "proposal loglik -59205.91796875\n",
      "updating map to  tensor(-59205.9180)\n",
      "\n",
      "step 111\n",
      "proposal loglik -58070.50390625\n",
      "updating map to  tensor(-58070.5039)\n",
      "\n",
      "step 112\n",
      "proposal loglik -57294.24609375\n",
      "updating map to  tensor(-57294.2461)\n",
      "\n",
      "step 114\n",
      "proposal loglik -56067.296875\n",
      "updating map to  tensor(-56067.2969)\n",
      "\n",
      "step 115\n",
      "proposal loglik -54376.734375\n",
      "updating map to  tensor(-54376.7344)\n",
      "\n",
      "step 116\n",
      "proposal loglik -53281.7421875\n",
      "updating map to  tensor(-53281.7422)\n",
      "\n",
      "step 117\n",
      "proposal loglik -52910.99609375\n",
      "updating map to  tensor(-52910.9961)\n",
      "\n",
      "step 118\n",
      "proposal loglik -51589.69921875\n",
      "updating map to  tensor(-51589.6992)\n",
      "\n",
      "step 120\n",
      "proposal loglik -50837.94921875\n",
      "updating map to  tensor(-50837.9492)\n",
      "\n",
      "step 122\n",
      "proposal loglik -50284.625\n",
      "updating map to  tensor(-50284.6250)\n",
      "\n",
      "step 123\n",
      "proposal loglik -49001.09765625\n",
      "updating map to  tensor(-49001.0977)\n",
      "\n",
      "step 124\n",
      "proposal loglik -47965.5\n",
      "updating map to  tensor(-47965.5000)\n",
      "\n",
      "step 125\n",
      "proposal loglik -47641.3671875\n",
      "updating map to  tensor(-47641.3672)\n",
      "\n",
      "step 128\n",
      "proposal loglik -46787.375\n",
      "updating map to  tensor(-46787.3750)\n",
      "\n",
      "step 129\n",
      "proposal loglik -45592.703125\n",
      "updating map to  tensor(-45592.7031)\n",
      "\n",
      "step 133\n",
      "proposal loglik -45334.0\n",
      "updating map to  tensor(-45334.)\n",
      "\n",
      "step 134\n",
      "proposal loglik -45251.80078125\n",
      "updating map to  tensor(-45251.8008)\n",
      "\n",
      "step 135\n",
      "proposal loglik -42007.5234375\n",
      "updating map to  tensor(-42007.5234)\n",
      "\n",
      "step 136\n",
      "proposal loglik -41651.796875\n",
      "updating map to  tensor(-41651.7969)\n",
      "\n",
      "step 139\n",
      "proposal loglik -41621.22265625\n",
      "updating map to  tensor(-41621.2227)\n",
      "\n",
      "step 140\n",
      "proposal loglik -41196.140625\n",
      "updating map to  tensor(-41196.1406)\n",
      "\n",
      "step 144\n",
      "proposal loglik -39177.89453125\n",
      "updating map to  tensor(-39177.8945)\n",
      "\n",
      "step 146\n",
      "proposal loglik -38258.93359375\n",
      "updating map to  tensor(-38258.9336)\n",
      "\n",
      "step 147\n",
      "proposal loglik -36372.53515625\n",
      "updating map to  tensor(-36372.5352)\n",
      "\n",
      "step 149\n",
      "proposal loglik -35443.57421875\n",
      "updating map to  tensor(-35443.5742)\n",
      "\n",
      "step 152\n",
      "proposal loglik -34811.16796875\n",
      "updating map to  tensor(-34811.1680)\n",
      "\n",
      "step 153\n",
      "proposal loglik -34648.09765625\n",
      "updating map to  tensor(-34648.0977)\n",
      "\n",
      "step 154\n",
      "proposal loglik -33050.6484375\n",
      "updating map to  tensor(-33050.6484)\n",
      "\n",
      "step 156\n",
      "proposal loglik -32771.29296875\n",
      "updating map to  tensor(-32771.2930)\n",
      "\n",
      "step 157\n",
      "proposal loglik -31794.044921875\n",
      "updating map to  tensor(-31794.0449)\n",
      "\n",
      "step 158\n",
      "proposal loglik -31602.021484375\n",
      "updating map to  tensor(-31602.0215)\n",
      "\n",
      "step 159\n",
      "proposal loglik -31180.662109375\n",
      "updating map to  tensor(-31180.6621)\n",
      "\n",
      "step 160\n",
      "proposal loglik -29741.205078125\n",
      "updating map to  tensor(-29741.2051)\n",
      "\n",
      "step 163\n",
      "proposal loglik -28007.509765625\n",
      "updating map to  tensor(-28007.5098)\n",
      "\n",
      "step 167\n",
      "proposal loglik -27392.421875\n",
      "updating map to  tensor(-27392.4219)\n",
      "\n",
      "step 168\n",
      "proposal loglik -27059.921875\n",
      "updating map to  tensor(-27059.9219)\n",
      "\n",
      "step 169\n",
      "proposal loglik -25299.185546875\n",
      "updating map to  tensor(-25299.1855)\n",
      "\n",
      "step 173\n",
      "proposal loglik -25009.462890625\n",
      "updating map to  tensor(-25009.4629)\n",
      "\n",
      "step 174\n",
      "proposal loglik -21494.630859375\n",
      "updating map to  tensor(-21494.6309)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 178\n",
      "proposal loglik -21049.787109375\n",
      "updating map to  tensor(-21049.7871)\n",
      "\n",
      "step 179\n",
      "proposal loglik -20924.35546875\n",
      "updating map to  tensor(-20924.3555)\n",
      "\n",
      "step 180\n",
      "proposal loglik -20532.52734375\n",
      "updating map to  tensor(-20532.5273)\n",
      "\n",
      "step 181\n",
      "proposal loglik -20368.9453125\n",
      "updating map to  tensor(-20368.9453)\n",
      "\n",
      "step 184\n",
      "proposal loglik -19104.744140625\n",
      "updating map to  tensor(-19104.7441)\n",
      "\n",
      "step 186\n",
      "proposal loglik -17637.576171875\n",
      "updating map to  tensor(-17637.5762)\n",
      "\n",
      "step 187\n",
      "proposal loglik -16725.70703125\n",
      "updating map to  tensor(-16725.7070)\n",
      "\n",
      "step 188\n",
      "proposal loglik -16496.845703125\n",
      "updating map to  tensor(-16496.8457)\n",
      "\n",
      "step 191\n",
      "proposal loglik -15201.0322265625\n",
      "updating map to  tensor(-15201.0322)\n",
      "\n",
      "step 192\n",
      "proposal loglik -14161.365234375\n",
      "updating map to  tensor(-14161.3652)\n",
      "\n",
      "step 193\n",
      "proposal loglik -13396.5693359375\n",
      "updating map to  tensor(-13396.5693)\n",
      "\n",
      "step 195\n",
      "proposal loglik -13302.296875\n",
      "updating map to  tensor(-13302.2969)\n",
      "\n",
      "step 198\n",
      "proposal loglik -12348.3486328125\n",
      "updating map to  tensor(-12348.3486)\n",
      "\n",
      "step 199\n",
      "proposal loglik -12244.1484375\n",
      "updating map to  tensor(-12244.1484)\n",
      "\n",
      "step 203\n",
      "proposal loglik -10498.4326171875\n",
      "updating map to  tensor(-10498.4326)\n",
      "\n",
      "step 204\n",
      "proposal loglik -10441.6064453125\n",
      "updating map to  tensor(-10441.6064)\n",
      "\n",
      "step 207\n",
      "proposal loglik -10001.087890625\n",
      "updating map to  tensor(-10001.0879)\n",
      "\n",
      "step 208\n",
      "proposal loglik -8507.3798828125\n",
      "updating map to  tensor(-8507.3799)\n",
      "\n",
      "step 210\n",
      "proposal loglik -7405.6455078125\n",
      "updating map to  tensor(-7405.6455)\n",
      "\n",
      "step 215\n",
      "proposal loglik -6886.4736328125\n",
      "updating map to  tensor(-6886.4736)\n",
      "\n",
      "step 216\n",
      "proposal loglik -6575.54833984375\n",
      "updating map to  tensor(-6575.5483)\n",
      "\n",
      "step 217\n",
      "proposal loglik -6488.90283203125\n",
      "updating map to  tensor(-6488.9028)\n",
      "\n",
      "step 218\n",
      "proposal loglik -6363.9912109375\n",
      "updating map to  tensor(-6363.9912)\n",
      "\n",
      "step 219\n",
      "proposal loglik -5967.1943359375\n",
      "updating map to  tensor(-5967.1943)\n",
      "\n",
      "step 220\n",
      "proposal loglik -5146.37548828125\n",
      "updating map to  tensor(-5146.3755)\n",
      "\n",
      "step 223\n",
      "proposal loglik -4921.8388671875\n",
      "updating map to  tensor(-4921.8389)\n",
      "\n",
      "step 225\n",
      "proposal loglik -4471.923828125\n",
      "updating map to  tensor(-4471.9238)\n",
      "\n",
      "step 254\n",
      "proposal loglik -4442.29833984375\n",
      "updating map to  tensor(-4442.2983)\n",
      "\n",
      "step 258\n",
      "proposal loglik -4377.7216796875\n",
      "updating map to  tensor(-4377.7217)\n",
      "\n",
      "step 266\n",
      "proposal loglik -4335.80712890625\n",
      "updating map to  tensor(-4335.8071)\n",
      "\n",
      "step 271\n",
      "proposal loglik -4155.69287109375\n",
      "updating map to  tensor(-4155.6929)\n",
      "\n",
      "step 281\n",
      "proposal loglik -4039.361572265625\n",
      "updating map to  tensor(-4039.3616)\n",
      "\n",
      "step 284\n",
      "proposal loglik -4029.005859375\n",
      "updating map to  tensor(-4029.0059)\n",
      "\n",
      "step 289\n",
      "proposal loglik -3968.81982421875\n",
      "updating map to  tensor(-3968.8198)\n",
      "\n",
      "step 293\n",
      "proposal loglik -3942.87060546875\n",
      "updating map to  tensor(-3942.8706)\n",
      "\n",
      "step 303\n",
      "proposal loglik -3733.404541015625\n",
      "updating map to  tensor(-3733.4045)\n",
      "\n",
      "step 305\n",
      "proposal loglik -3692.026123046875\n",
      "updating map to  tensor(-3692.0261)\n",
      "\n",
      "step 309\n",
      "proposal loglik -3629.85888671875\n",
      "updating map to  tensor(-3629.8589)\n",
      "\n",
      "step 315\n",
      "proposal loglik -3588.20263671875\n",
      "updating map to  tensor(-3588.2026)\n",
      "\n",
      "step 317\n",
      "proposal loglik -3578.716064453125\n",
      "updating map to  tensor(-3578.7161)\n",
      "\n",
      "step 321\n",
      "proposal loglik -3559.78466796875\n",
      "updating map to  tensor(-3559.7847)\n",
      "\n",
      "step 326\n",
      "proposal loglik -3524.347412109375\n",
      "updating map to  tensor(-3524.3474)\n",
      "\n",
      "step 341\n",
      "proposal loglik -3458.747314453125\n",
      "updating map to  tensor(-3458.7473)\n",
      "\n",
      "step 360\n",
      "proposal loglik -3417.728515625\n",
      "updating map to  tensor(-3417.7285)\n",
      "\n",
      "step 370\n",
      "proposal loglik -3410.4404296875\n",
      "updating map to  tensor(-3410.4404)\n",
      "\n",
      "step 387\n",
      "proposal loglik -3371.533935546875\n",
      "updating map to  tensor(-3371.5339)\n",
      "\n",
      "step 392\n",
      "proposal loglik -3246.632568359375\n",
      "updating map to  tensor(-3246.6326)\n",
      "\n",
      "step 401\n",
      "proposal loglik -3222.527587890625\n",
      "updating map to  tensor(-3222.5276)\n",
      "\n",
      "step 411\n",
      "proposal loglik -3206.916015625\n",
      "updating map to  tensor(-3206.9160)\n",
      "\n",
      "step 423\n",
      "proposal loglik -3114.28759765625\n",
      "updating map to  tensor(-3114.2876)\n",
      "\n",
      "step 429\n",
      "proposal loglik -3112.19189453125\n",
      "updating map to  tensor(-3112.1919)\n",
      "\n",
      "step 434\n",
      "proposal loglik -3102.39404296875\n",
      "updating map to  tensor(-3102.3940)\n",
      "\n",
      "step 466\n",
      "proposal loglik -2967.57568359375\n",
      "updating map to  tensor(-2967.5757)\n",
      "\n",
      "step 472\n",
      "proposal loglik -2929.651611328125\n",
      "updating map to  tensor(-2929.6516)\n",
      "\n",
      "step 491\n",
      "proposal loglik -2884.958251953125\n",
      "updating map to  tensor(-2884.9583)\n",
      "\n",
      "step 499\n",
      "proposal loglik -2849.458251953125\n",
      "updating map to  tensor(-2849.4583)\n",
      "\n",
      "step 500\n",
      "proposal loglik -2724.896240234375\n",
      "updating map to  tensor(-2724.8962)\n",
      "\n",
      "step 501\n",
      "proposal loglik -2709.045654296875\n",
      "updating map to  tensor(-2709.0457)\n",
      "\n",
      "step 507\n",
      "proposal loglik -2679.798583984375\n",
      "updating map to  tensor(-2679.7986)\n",
      "\n",
      "step 522\n",
      "proposal loglik -2669.13818359375\n",
      "updating map to  tensor(-2669.1382)\n",
      "\n",
      "step 525\n",
      "proposal loglik -2429.881103515625\n",
      "updating map to  tensor(-2429.8811)\n",
      "\n",
      "step 534\n",
      "proposal loglik -2404.713134765625\n",
      "updating map to  tensor(-2404.7131)\n",
      "\n",
      "step 541\n",
      "proposal loglik -2404.16650390625\n",
      "updating map to  tensor(-2404.1665)\n",
      "\n",
      "step 543\n",
      "proposal loglik -2358.60546875\n",
      "updating map to  tensor(-2358.6055)\n",
      "\n",
      "step 547\n",
      "proposal loglik -2321.554443359375\n",
      "updating map to  tensor(-2321.5544)\n",
      "\n",
      "step 548\n",
      "proposal loglik -2275.842529296875\n",
      "updating map to  tensor(-2275.8425)\n",
      "\n",
      "step 549\n",
      "proposal loglik -1983.63623046875\n",
      "updating map to  tensor(-1983.6362)\n",
      "\n",
      "step 555\n",
      "proposal loglik -1975.29638671875\n",
      "updating map to  tensor(-1975.2964)\n",
      "\n",
      "step 558\n",
      "proposal loglik -1873.50927734375\n",
      "updating map to  tensor(-1873.5093)\n",
      "\n",
      "step 564\n",
      "proposal loglik -1865.23486328125\n",
      "updating map to  tensor(-1865.2349)\n",
      "\n",
      "step 565\n",
      "proposal loglik -1812.9842529296875\n",
      "updating map to  tensor(-1812.9843)\n",
      "\n",
      "step 569\n",
      "proposal loglik -1753.5255126953125\n",
      "updating map to  tensor(-1753.5255)\n",
      "\n",
      "step 575\n",
      "proposal loglik -1648.8829345703125\n",
      "updating map to  tensor(-1648.8829)\n",
      "\n",
      "step 578\n",
      "proposal loglik -1634.5863037109375\n",
      "updating map to  tensor(-1634.5863)\n",
      "\n",
      "step 579\n",
      "proposal loglik -1608.5572509765625\n",
      "updating map to  tensor(-1608.5573)\n",
      "\n",
      "step 582\n",
      "proposal loglik -1456.954833984375\n",
      "updating map to  tensor(-1456.9548)\n",
      "\n",
      "step 586\n",
      "proposal loglik -1441.3834228515625\n",
      "updating map to  tensor(-1441.3834)\n",
      "\n",
      "step 590\n",
      "proposal loglik -1366.4970703125\n",
      "updating map to  tensor(-1366.4971)\n",
      "\n",
      "step 591\n",
      "proposal loglik -1287.2789306640625\n",
      "updating map to  tensor(-1287.2789)\n",
      "\n",
      "step 594\n",
      "proposal loglik -1166.72265625\n",
      "updating map to  tensor(-1166.7227)\n",
      "\n",
      "step 611\n",
      "proposal loglik -1116.142333984375\n",
      "updating map to  tensor(-1116.1423)\n",
      "\n",
      "step 612\n",
      "proposal loglik -955.002685546875\n",
      "updating map to  tensor(-955.0027)\n",
      "\n",
      "step 615\n",
      "proposal loglik -948.6101684570312\n",
      "updating map to  tensor(-948.6102)\n",
      "\n",
      "step 617\n",
      "proposal loglik -872.1856689453125\n",
      "updating map to  tensor(-872.1857)\n",
      "\n",
      "step 618\n",
      "proposal loglik -867.8946533203125\n",
      "updating map to  tensor(-867.8947)\n",
      "\n",
      "step 622\n",
      "proposal loglik -711.05712890625\n",
      "updating map to  tensor(-711.0571)\n",
      "\n",
      "step 633\n",
      "proposal loglik -623.5628051757812\n",
      "updating map to  tensor(-623.5628)\n",
      "\n",
      "step 639\n",
      "proposal loglik -571.960693359375\n",
      "updating map to  tensor(-571.9607)\n",
      "\n",
      "step 645\n",
      "proposal loglik -558.2147216796875\n",
      "updating map to  tensor(-558.2147)\n",
      "\n",
      "step 656\n",
      "proposal loglik -523.5053100585938\n",
      "updating map to  tensor(-523.5053)\n",
      "\n",
      "step 657\n",
      "proposal loglik -503.84356689453125\n",
      "updating map to  tensor(-503.8436)\n",
      "\n",
      "step 658\n",
      "proposal loglik -475.14434814453125\n",
      "updating map to  tensor(-475.1443)\n",
      "\n",
      "step 669\n",
      "proposal loglik -457.9155578613281\n",
      "updating map to  tensor(-457.9156)\n",
      "\n",
      "step 673\n",
      "proposal loglik -427.0111083984375\n",
      "updating map to  tensor(-427.0111)\n",
      "\n",
      "step 682\n",
      "proposal loglik -372.6824951171875\n",
      "updating map to  tensor(-372.6825)\n",
      "\n",
      "step 687\n",
      "proposal loglik -334.2051086425781\n",
      "updating map to  tensor(-334.2051)\n",
      "\n",
      "step 688\n",
      "proposal loglik -278.52557373046875\n",
      "updating map to  tensor(-278.5256)\n",
      "\n",
      "step 725\n",
      "proposal loglik -273.4499816894531\n",
      "updating map to  tensor(-273.4500)\n",
      "\n",
      "step 726\n",
      "proposal loglik -137.92494201660156\n",
      "updating map to  tensor(-137.9249)\n",
      "\n",
      "step 731\n",
      "proposal loglik -106.87371826171875\n",
      "updating map to  tensor(-106.8737)\n",
      "\n",
      "step 743\n",
      "proposal loglik -79.54113006591797\n",
      "updating map to  tensor(-79.5411)\n",
      "\n",
      "step 802\n",
      "proposal loglik -68.97317504882812\n",
      "updating map to  tensor(-68.9732)\n",
      "\n",
      "step 818\n",
      "proposal loglik -44.611488342285156\n",
      "updating map to  tensor(-44.6115)\n",
      "\n",
      "step 822\n",
      "proposal loglik -42.35265350341797\n",
      "updating map to  tensor(-42.3527)\n",
      "\n",
      "step 831\n",
      "proposal loglik -41.4132080078125\n",
      "updating map to  tensor(-41.4132)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 869\n",
      "proposal loglik -39.00806427001953\n",
      "updating map to  tensor(-39.0081)\n",
      "\n",
      "step 1001\n",
      "proposal loglik -38.72477722167969\n",
      "updating map to  tensor(-38.7248)\n",
      "\n",
      "step 1472\n",
      "proposal loglik -36.786277770996094\n",
      "updating map to  tensor(-36.7863)\n",
      "\n",
      "step 1485\n",
      "proposal loglik -36.74752426147461\n",
      "updating map to  tensor(-36.7475)\n",
      "\n",
      "step 1552\n",
      "proposal loglik -36.29877471923828\n",
      "updating map to  tensor(-36.2988)\n",
      "\n",
      "step 1592\n",
      "proposal loglik -34.68990707397461\n",
      "updating map to  tensor(-34.6899)\n",
      "\n",
      "step 1640\n",
      "proposal loglik -34.4594612121582\n",
      "updating map to  tensor(-34.4595)\n",
      "\n",
      "step 1663\n",
      "proposal loglik -33.729515075683594\n",
      "updating map to  tensor(-33.7295)\n",
      "\n",
      "step 1892\n",
      "proposal loglik -33.410186767578125\n",
      "updating map to  tensor(-33.4102)\n",
      "\n",
      "step 1931\n",
      "proposal loglik -33.348838806152344\n",
      "updating map to  tensor(-33.3488)\n",
      "\n",
      "step 2138\n",
      "proposal loglik -32.33916473388672\n",
      "updating map to  tensor(-32.3392)\n",
      "\n",
      "step 2176\n",
      "proposal loglik -32.312355041503906\n",
      "updating map to  tensor(-32.3124)\n",
      "\n",
      "step 2181\n",
      "proposal loglik -31.10079574584961\n",
      "updating map to  tensor(-31.1008)\n",
      "\n",
      "step 2341\n",
      "proposal loglik -30.14925193786621\n",
      "updating map to  tensor(-30.1493)\n",
      "\n",
      "step 2355\n",
      "proposal loglik -29.931821823120117\n",
      "updating map to  tensor(-29.9318)\n",
      "\n",
      "step 2385\n",
      "proposal loglik -29.847070693969727\n",
      "updating map to  tensor(-29.8471)\n",
      "\n",
      "step 3021\n",
      "proposal loglik -29.357004165649414\n",
      "updating map to  tensor(-29.3570)\n",
      "\n",
      "step 3079\n",
      "proposal loglik -29.01865577697754\n",
      "updating map to  tensor(-29.0187)\n",
      "\n",
      "step 3154\n",
      "proposal loglik -27.732683181762695\n",
      "updating map to  tensor(-27.7327)\n",
      "\n",
      "step 3175\n",
      "proposal loglik -27.054553985595703\n",
      "updating map to  tensor(-27.0546)\n",
      "\n",
      "step 3866\n",
      "proposal loglik -26.84809684753418\n",
      "updating map to  tensor(-26.8481)\n",
      "\n",
      "step 3905\n",
      "proposal loglik -26.645179748535156\n",
      "updating map to  tensor(-26.6452)\n",
      "\n",
      "step 3914\n",
      "proposal loglik -25.863521575927734\n",
      "updating map to  tensor(-25.8635)\n",
      "\n",
      "step 4251\n",
      "proposal loglik -24.968591690063477\n",
      "updating map to  tensor(-24.9686)\n",
      "\n",
      "step 4289\n",
      "proposal loglik -24.819276809692383\n",
      "updating map to  tensor(-24.8193)\n",
      "\n",
      "step 4330\n",
      "proposal loglik -24.68889617919922\n",
      "updating map to  tensor(-24.6889)\n",
      "\n",
      "step 4414\n",
      "proposal loglik -23.994478225708008\n",
      "updating map to  tensor(-23.9945)\n",
      "\n",
      "step 4464\n",
      "proposal loglik -23.79755401611328\n",
      "updating map to  tensor(-23.7976)\n",
      "\n",
      "step 5064\n",
      "proposal loglik -23.256561279296875\n",
      "updating map to  tensor(-23.2566)\n",
      "\n",
      "step 5172\n",
      "proposal loglik -23.08302879333496\n",
      "updating map to  tensor(-23.0830)\n",
      "\n",
      "step 5439\n",
      "proposal loglik -23.03593635559082\n",
      "updating map to  tensor(-23.0359)\n",
      "\n",
      "step 5488\n",
      "proposal loglik -22.2952880859375\n",
      "updating map to  tensor(-22.2953)\n",
      "\n",
      "step 5655\n",
      "proposal loglik -21.22913932800293\n",
      "updating map to  tensor(-21.2291)\n",
      "\n",
      "step 6125\n",
      "proposal loglik -21.164234161376953\n",
      "updating map to  tensor(-21.1642)\n",
      "\n",
      "step 6302\n",
      "proposal loglik -20.891515731811523\n",
      "updating map to  tensor(-20.8915)\n",
      "\n",
      "step 6368\n",
      "proposal loglik -20.84722137451172\n",
      "updating map to  tensor(-20.8472)\n",
      "\n",
      "step 6674\n",
      "proposal loglik -20.426774978637695\n",
      "updating map to  tensor(-20.4268)\n",
      "\n",
      "step 6809\n",
      "proposal loglik -20.214536666870117\n",
      "updating map to  tensor(-20.2145)\n",
      "\n",
      "step 7420\n",
      "proposal loglik -20.156869888305664\n",
      "updating map to  tensor(-20.1569)\n",
      "\n",
      "step 8001\n",
      "proposal loglik -20.0379638671875\n",
      "updating map to  tensor(-20.0380)\n",
      "\n",
      "step 8054\n",
      "proposal loglik -19.887998580932617\n",
      "updating map to  tensor(-19.8880)\n",
      "\n",
      "step 8294\n",
      "proposal loglik -19.839996337890625\n",
      "updating map to  tensor(-19.8400)\n",
      "\n",
      "step 8953\n",
      "proposal loglik -19.749475479125977\n",
      "updating map to  tensor(-19.7495)\n",
      "\n",
      "step 9132\n",
      "proposal loglik -19.591228485107422\n",
      "updating map to  tensor(-19.5912)\n",
      "\n",
      "step 10189\n",
      "proposal loglik -19.21564292907715\n",
      "updating map to  tensor(-19.2156)\n",
      "\n",
      "step 10445\n",
      "proposal loglik -18.578474044799805\n",
      "updating map to  tensor(-18.5785)\n",
      "\n",
      "step 10725\n",
      "proposal loglik -18.521936416625977\n",
      "updating map to  tensor(-18.5219)\n",
      "\n",
      "step 10826\n",
      "proposal loglik -18.18317222595215\n",
      "updating map to  tensor(-18.1832)\n",
      "\n",
      "step 11825\n",
      "proposal loglik -17.971359252929688\n",
      "updating map to  tensor(-17.9714)\n",
      "\n",
      "step 15947\n",
      "proposal loglik -17.5146427154541\n",
      "updating map to  tensor(-17.5146)\n",
      "\n",
      "step 16030\n",
      "proposal loglik -17.219539642333984\n",
      "updating map to  tensor(-17.2195)\n",
      "\n",
      "step 16545\n",
      "proposal loglik -17.01371192932129\n",
      "updating map to  tensor(-17.0137)\n",
      "\n",
      "step 19908\n",
      "proposal loglik -16.70114517211914\n",
      "updating map to  tensor(-16.7011)\n",
      "\n",
      "step 21563\n",
      "proposal loglik -16.547462463378906\n",
      "updating map to  tensor(-16.5475)\n",
      "\n",
      "step 22565\n",
      "proposal loglik -16.445755004882812\n",
      "updating map to  tensor(-16.4458)\n",
      "\n",
      "step 23200\n",
      "proposal loglik -16.110809326171875\n",
      "updating map to  tensor(-16.1108)\n",
      "\n",
      "step 24527\n",
      "proposal loglik -16.094083786010742\n",
      "updating map to  tensor(-16.0941)\n",
      "\n",
      "step 24621\n",
      "proposal loglik -16.026342391967773\n",
      "updating map to  tensor(-16.0263)\n",
      "\n",
      "step 24701\n",
      "proposal loglik -15.945313453674316\n",
      "updating map to  tensor(-15.9453)\n",
      "\n",
      "step 30933\n",
      "proposal loglik -15.86852741241455\n",
      "updating map to  tensor(-15.8685)\n",
      "\n",
      "step 30986\n",
      "proposal loglik -15.764798164367676\n",
      "updating map to  tensor(-15.7648)\n",
      "\n",
      "step 31112\n",
      "proposal loglik -15.74760913848877\n",
      "updating map to  tensor(-15.7476)\n",
      "\n",
      "step 32606\n",
      "proposal loglik -15.740702629089355\n",
      "updating map to  tensor(-15.7407)\n",
      "\n",
      "step 34388\n",
      "proposal loglik -15.549623489379883\n",
      "updating map to  tensor(-15.5496)\n",
      "\n",
      "step 34611\n",
      "proposal loglik -15.498592376708984\n",
      "updating map to  tensor(-15.4986)\n",
      "\n",
      "step 35236\n",
      "proposal loglik -15.43856143951416\n",
      "updating map to  tensor(-15.4386)\n",
      "\n",
      "step 35839\n",
      "proposal loglik -15.41819953918457\n",
      "updating map to  tensor(-15.4182)\n",
      "num rejects 96325\n",
      "num accepts 3675\n",
      "0 0.005204159766435623 0.0\n",
      "1 -91.7627300620079 0.0\n",
      "2 -19.35610906034708 0.0\n",
      "3 0.6074839644134045 0.0\n",
      "4 1.2765107657760382 3.0\n",
      "5 4.197714276611805 8.0\n",
      "6 4.098532904870808 8.0\n",
      "7 4.985733145847917 10.0\n",
      "8 7.66737846750766 14.0\n",
      "9 9.145829673856497 16.0\n",
      "10 9.653305053710938 17.0\n",
      "11 12.313080267980695 19.0\n",
      "12 9.82299084495753 20.0\n",
      "13 9.984336479566991 21.0\n",
      "14 10.022744368761778 23.0\n",
      "15 15.679517510347068 397.0\n",
      "16 20.37821560818702 401.0\n",
      "17 25.903863100335002 417.0\n",
      "18 27.18408938124776 431.0\n"
     ]
    }
   ],
   "source": [
    "# Now we download a pretrained network to form \\phi(s) the state features where the reward is now w^T \\phi(s)\n",
    "num_mcmc_steps = 100000#', default=2000, type = int, help=\"number of proposals to generate for MCMC\")\n",
    "mcmc_step_size = 0.001\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "reward_net = EmbeddingNet(encoding_dims)\n",
    "reward_net.load_state_dict(torch.load(pretrained_network, map_location=device))\n",
    "#reinitialize last layer\n",
    "num_features = reward_net.fc2.in_features\n",
    "\n",
    "print(\"reward is linear combination of \", num_features, \"features\")\n",
    "reward_net.fc2 = nn.Linear(num_features, 1, bias=False) #last layer just outputs the scalar reward = w^T \\phi(s)\n",
    "reward_net.to(device)\n",
    "#freeze all weights so there are no gradients (we'll manually update the last layer via proposals so no grads required)\n",
    "for param in reward_net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#get num_demos by num_features + 1 (bias) numpy array with (un-discounted) feature counts from pretrained network\n",
    "demo_cnts = generate_feature_counts(demonstrations, reward_net)\n",
    "print(\"demo counts\")\n",
    "print(demo_cnts)\n",
    "#just need index tuples (i,j) denoting j is preferred to i. Assuming all pairwise prefs for now\n",
    "#check if really better, there might be ties!\n",
    "pairwise_prefs = []\n",
    "for i in range(len(demonstrations)):\n",
    "    for j in range(i+1, len(demonstrations)):\n",
    "        if sorted_returns[i] < sorted_returns[j]:\n",
    "            pairwise_prefs.append((i,j))\n",
    "        else: # they are equal\n",
    "            print(\"not using equal prefs\", i, j, sorted_returns[i], sorted_returns[j])\n",
    "            #pairwise_prefs.append((i,j))\n",
    "            #pairwise_prefs.append((j,i))\n",
    "\n",
    "\n",
    "#run random search over weights\n",
    "#best_reward = random_search(reward_net, demonstrations, 40, stdev = 0.01)\n",
    "best_reward_lastlayer = mcmc_map_search(reward_net, demonstrations, pairwise_prefs, demo_cnts, num_mcmc_steps, mcmc_step_size, weight_outputfile, weight_init)\n",
    "#turn this into a full network\n",
    "best_reward = EmbeddingNet(encoding_dims)\n",
    "#best_reward.fc2 = nn.Linear(num_features, 1, bias=False)\n",
    "best_reward.load_state_dict(torch.load(pretrained_network, map_location=device))\n",
    "best_reward.fc2 = best_reward_lastlayer\n",
    "\n",
    "best_reward.to(device)\n",
    "#print(best_reward.state_dict())\n",
    "#save best reward network\n",
    "#torch.save(best_reward.state_dict(), args.map_reward_model_path)\n",
    "demo_pairs, preference_labels = create_mcmc_likelihood_data(demonstrations)\n",
    "print_traj_returns(best_reward, demonstrations)\n",
    "\n",
    "\n",
    "#add random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
